{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of P2S10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkdivya/EVA/blob/master/Phase2/10.ENDGAME/P2S10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        "Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab_type": "code",
        "outputId": "4a2d8ad7-9867-4d12-8358-80552664b0a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/c3/3b9bbc274ce7b056c3b01c1412779434dfe1cba200d44ffb1cf4fa5ba08c/pybullet-2.7.3-cp36-cp36m-manylinux1_x86_64.whl (95.0MB)\n",
            "\u001b[K     |████████████████████████████████| 95.0MB 43kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-2.7.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "outputId": "e4035b22-2ce9-437d-8902-b9f98ced4859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "outputId": "15e46418-c223-408d-f4c1-4c81629422aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "outputId": "96229d70-c7cf-455b-b237-6b5380e584f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804960\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "outputId": "4642183a-800f-40c2-c714-535ad60b3c7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 494.068675124317\n",
            "Total Timesteps: 1107 Episode Num: 2 Reward: 39.725063001803946\n",
            "Total Timesteps: 2107 Episode Num: 3 Reward: 513.5019926212249\n",
            "Total Timesteps: 3107 Episode Num: 4 Reward: 467.07174948917833\n",
            "Total Timesteps: 4107 Episode Num: 5 Reward: 505.83288073272007\n",
            "Total Timesteps: 5107 Episode Num: 6 Reward: 493.48173335686874\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 175.467364\n",
            "---------------------------------------\n",
            "Total Timesteps: 6107 Episode Num: 7 Reward: 498.42856707188287\n",
            "Total Timesteps: 7107 Episode Num: 8 Reward: 494.25889645556816\n",
            "Total Timesteps: 8107 Episode Num: 9 Reward: 506.00406210679336\n",
            "Total Timesteps: 9107 Episode Num: 10 Reward: 398.6771842889664\n",
            "Total Timesteps: 9178 Episode Num: 11 Reward: 29.950832148868354\n",
            "Total Timesteps: 10178 Episode Num: 12 Reward: 415.7698372022337\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 179.860390\n",
            "---------------------------------------\n",
            "Total Timesteps: 11178 Episode Num: 13 Reward: 287.69177890152986\n",
            "Total Timesteps: 12178 Episode Num: 14 Reward: 118.39772213260422\n",
            "Total Timesteps: 13178 Episode Num: 15 Reward: 115.5479161818525\n",
            "Total Timesteps: 14178 Episode Num: 16 Reward: 192.06513208482377\n",
            "Total Timesteps: 15178 Episode Num: 17 Reward: 50.98773471521275\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 130.903161\n",
            "---------------------------------------\n",
            "Total Timesteps: 16178 Episode Num: 18 Reward: 99.04512767516896\n",
            "Total Timesteps: 17178 Episode Num: 19 Reward: 250.64398157834947\n",
            "Total Timesteps: 18178 Episode Num: 20 Reward: 289.71588558947894\n",
            "Total Timesteps: 19178 Episode Num: 21 Reward: 288.088616144828\n",
            "Total Timesteps: 20178 Episode Num: 22 Reward: 300.47068836669837\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 289.122901\n",
            "---------------------------------------\n",
            "Total Timesteps: 21178 Episode Num: 23 Reward: 212.05049819059073\n",
            "Total Timesteps: 22178 Episode Num: 24 Reward: 204.8929328519531\n",
            "Total Timesteps: 23178 Episode Num: 25 Reward: 290.6987157596011\n",
            "Total Timesteps: 24178 Episode Num: 26 Reward: 392.3721315303405\n",
            "Total Timesteps: 24341 Episode Num: 27 Reward: 19.8559734784261\n",
            "Total Timesteps: 25341 Episode Num: 28 Reward: 326.135509817038\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 163.817579\n",
            "---------------------------------------\n",
            "Total Timesteps: 26341 Episode Num: 29 Reward: 145.42313705547585\n",
            "Total Timesteps: 27341 Episode Num: 30 Reward: 310.16073182855655\n",
            "Total Timesteps: 28341 Episode Num: 31 Reward: 152.08303390862096\n",
            "Total Timesteps: 28370 Episode Num: 32 Reward: -0.9610065503102183\n",
            "Total Timesteps: 28398 Episode Num: 33 Reward: -3.156662008486709\n",
            "Total Timesteps: 28424 Episode Num: 34 Reward: -0.9229722277470519\n",
            "Total Timesteps: 28445 Episode Num: 35 Reward: -0.6948259126768983\n",
            "Total Timesteps: 28465 Episode Num: 36 Reward: -0.9028238706405989\n",
            "Total Timesteps: 28485 Episode Num: 37 Reward: -0.24985840885295874\n",
            "Total Timesteps: 28505 Episode Num: 38 Reward: 1.816196221163786\n",
            "Total Timesteps: 28525 Episode Num: 39 Reward: 0.6385541992954478\n",
            "Total Timesteps: 28545 Episode Num: 40 Reward: 0.3049224776218624\n",
            "Total Timesteps: 28565 Episode Num: 41 Reward: 0.24849879006452635\n",
            "Total Timesteps: 28585 Episode Num: 42 Reward: 1.2306849165843334\n",
            "Total Timesteps: 28605 Episode Num: 43 Reward: 1.0950618074336513\n",
            "Total Timesteps: 28625 Episode Num: 44 Reward: 0.5720878662267128\n",
            "Total Timesteps: 28645 Episode Num: 45 Reward: -1.7968735049412397\n",
            "Total Timesteps: 28665 Episode Num: 46 Reward: -0.5589644898464541\n",
            "Total Timesteps: 28685 Episode Num: 47 Reward: -0.7314359122978107\n",
            "Total Timesteps: 28705 Episode Num: 48 Reward: 0.7778329007948144\n",
            "Total Timesteps: 28725 Episode Num: 49 Reward: 0.46316638748368266\n",
            "Total Timesteps: 28745 Episode Num: 50 Reward: 0.29601996171960776\n",
            "Total Timesteps: 29745 Episode Num: 51 Reward: 242.77434907207572\n",
            "Total Timesteps: 30745 Episode Num: 52 Reward: 269.50374247418677\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 7.492488\n",
            "---------------------------------------\n",
            "Total Timesteps: 30782 Episode Num: 53 Reward: 7.548875773634312\n",
            "Total Timesteps: 30821 Episode Num: 54 Reward: 3.3605085897204843\n",
            "Total Timesteps: 31346 Episode Num: 55 Reward: 74.31265501611516\n",
            "Total Timesteps: 32346 Episode Num: 56 Reward: 170.46962841041207\n",
            "Total Timesteps: 32692 Episode Num: 57 Reward: 125.4329289007349\n",
            "Total Timesteps: 33692 Episode Num: 58 Reward: 120.37643692865534\n",
            "Total Timesteps: 33815 Episode Num: 59 Reward: 17.92834988175996\n",
            "Total Timesteps: 34815 Episode Num: 60 Reward: 340.5662028600174\n",
            "Total Timesteps: 35815 Episode Num: 61 Reward: 132.48558381293907\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 215.059879\n",
            "---------------------------------------\n",
            "Total Timesteps: 36815 Episode Num: 62 Reward: 149.53392775465875\n",
            "Total Timesteps: 37815 Episode Num: 63 Reward: 365.9470952234189\n",
            "Total Timesteps: 38815 Episode Num: 64 Reward: 421.54392558422745\n",
            "Total Timesteps: 39815 Episode Num: 65 Reward: 118.74121530271758\n",
            "Total Timesteps: 40815 Episode Num: 66 Reward: 245.49576931403328\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 394.980591\n",
            "---------------------------------------\n",
            "Total Timesteps: 41815 Episode Num: 67 Reward: 199.72913998357402\n",
            "Total Timesteps: 42815 Episode Num: 68 Reward: 257.2133472228908\n",
            "Total Timesteps: 43815 Episode Num: 69 Reward: 248.13087278762927\n",
            "Total Timesteps: 44815 Episode Num: 70 Reward: 291.0712789287601\n",
            "Total Timesteps: 45815 Episode Num: 71 Reward: 472.1113311099741\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 325.506945\n",
            "---------------------------------------\n",
            "Total Timesteps: 45863 Episode Num: 72 Reward: -22.669940353440968\n",
            "Total Timesteps: 46045 Episode Num: 73 Reward: 24.60817867383982\n",
            "Total Timesteps: 47045 Episode Num: 74 Reward: 404.32783785005614\n",
            "Total Timesteps: 48045 Episode Num: 75 Reward: 258.65395508195326\n",
            "Total Timesteps: 49045 Episode Num: 76 Reward: 512.2864456370227\n",
            "Total Timesteps: 50045 Episode Num: 77 Reward: 661.1854103241379\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 573.158677\n",
            "---------------------------------------\n",
            "Total Timesteps: 51045 Episode Num: 78 Reward: 491.81378019151447\n",
            "Total Timesteps: 52045 Episode Num: 79 Reward: 726.9786355534799\n",
            "Total Timesteps: 53045 Episode Num: 80 Reward: 491.1202156704579\n",
            "Total Timesteps: 54045 Episode Num: 81 Reward: 546.1296867264994\n",
            "Total Timesteps: 55045 Episode Num: 82 Reward: 538.958297865016\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 515.184712\n",
            "---------------------------------------\n",
            "Total Timesteps: 56045 Episode Num: 83 Reward: 565.1702677640768\n",
            "Total Timesteps: 57045 Episode Num: 84 Reward: 506.49421146129407\n",
            "Total Timesteps: 58045 Episode Num: 85 Reward: 657.6709502865509\n",
            "Total Timesteps: 59045 Episode Num: 86 Reward: 396.17402614963777\n",
            "Total Timesteps: 60045 Episode Num: 87 Reward: 632.9447675049845\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 474.221368\n",
            "---------------------------------------\n",
            "Total Timesteps: 61045 Episode Num: 88 Reward: 613.6773014539855\n",
            "Total Timesteps: 62045 Episode Num: 89 Reward: 611.0091788371353\n",
            "Total Timesteps: 63045 Episode Num: 90 Reward: 521.1293889629859\n",
            "Total Timesteps: 64045 Episode Num: 91 Reward: 280.9848411091657\n",
            "Total Timesteps: 65045 Episode Num: 92 Reward: 528.2322657761772\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 307.559835\n",
            "---------------------------------------\n",
            "Total Timesteps: 66045 Episode Num: 93 Reward: 416.843348119535\n",
            "Total Timesteps: 67045 Episode Num: 94 Reward: 447.1654685504135\n",
            "Total Timesteps: 68045 Episode Num: 95 Reward: 624.2498288863741\n",
            "Total Timesteps: 69045 Episode Num: 96 Reward: 348.3566054343328\n",
            "Total Timesteps: 70045 Episode Num: 97 Reward: 568.7676346070488\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 536.730323\n",
            "---------------------------------------\n",
            "Total Timesteps: 71045 Episode Num: 98 Reward: 386.68677274835284\n",
            "Total Timesteps: 72045 Episode Num: 99 Reward: 509.23770170855914\n",
            "Total Timesteps: 73045 Episode Num: 100 Reward: 342.6648213529305\n",
            "Total Timesteps: 74045 Episode Num: 101 Reward: 537.1620608868727\n",
            "Total Timesteps: 75045 Episode Num: 102 Reward: 556.8358305886591\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 536.836149\n",
            "---------------------------------------\n",
            "Total Timesteps: 76045 Episode Num: 103 Reward: 521.4950695634932\n",
            "Total Timesteps: 77045 Episode Num: 104 Reward: 354.5496772449148\n",
            "Total Timesteps: 78045 Episode Num: 105 Reward: 224.06840584077756\n",
            "Total Timesteps: 79045 Episode Num: 106 Reward: 372.54608238982723\n",
            "Total Timesteps: 80045 Episode Num: 107 Reward: 764.0674767513386\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 200.513530\n",
            "---------------------------------------\n",
            "Total Timesteps: 81045 Episode Num: 108 Reward: 269.5837852677304\n",
            "Total Timesteps: 82045 Episode Num: 109 Reward: 405.42634189313816\n",
            "Total Timesteps: 83045 Episode Num: 110 Reward: 281.9662531872231\n",
            "Total Timesteps: 84045 Episode Num: 111 Reward: 643.2886111748978\n",
            "Total Timesteps: 85045 Episode Num: 112 Reward: 492.3333535747103\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 377.036632\n",
            "---------------------------------------\n",
            "Total Timesteps: 86045 Episode Num: 113 Reward: 509.88381517604694\n",
            "Total Timesteps: 87045 Episode Num: 114 Reward: 518.8150908600882\n",
            "Total Timesteps: 88045 Episode Num: 115 Reward: 623.1411284537453\n",
            "Total Timesteps: 89045 Episode Num: 116 Reward: 295.74596994967317\n",
            "Total Timesteps: 90045 Episode Num: 117 Reward: 650.6928303431943\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 439.159183\n",
            "---------------------------------------\n",
            "Total Timesteps: 91045 Episode Num: 118 Reward: 440.6696242535917\n",
            "Total Timesteps: 92045 Episode Num: 119 Reward: 599.0935606545453\n",
            "Total Timesteps: 93045 Episode Num: 120 Reward: 741.6196808243075\n",
            "Total Timesteps: 94045 Episode Num: 121 Reward: 435.60721245684067\n",
            "Total Timesteps: 95045 Episode Num: 122 Reward: 720.5421873680734\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 559.606579\n",
            "---------------------------------------\n",
            "Total Timesteps: 96045 Episode Num: 123 Reward: 490.8447031530743\n",
            "Total Timesteps: 97045 Episode Num: 124 Reward: 366.56467257968706\n",
            "Total Timesteps: 98045 Episode Num: 125 Reward: 387.1840161621043\n",
            "Total Timesteps: 99045 Episode Num: 126 Reward: 761.9442036425595\n",
            "Total Timesteps: 100045 Episode Num: 127 Reward: 471.27442969532166\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 563.944507\n",
            "---------------------------------------\n",
            "Total Timesteps: 101045 Episode Num: 128 Reward: 527.9607739346612\n",
            "Total Timesteps: 102045 Episode Num: 129 Reward: 523.0846468609416\n",
            "Total Timesteps: 103045 Episode Num: 130 Reward: 374.7779816111766\n",
            "Total Timesteps: 104045 Episode Num: 131 Reward: 571.0638107492543\n",
            "Total Timesteps: 105045 Episode Num: 132 Reward: 624.5430046895256\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 684.810653\n",
            "---------------------------------------\n",
            "Total Timesteps: 106045 Episode Num: 133 Reward: 512.9585003171827\n",
            "Total Timesteps: 107045 Episode Num: 134 Reward: 630.2092439922434\n",
            "Total Timesteps: 108045 Episode Num: 135 Reward: 502.71435044028254\n",
            "Total Timesteps: 109045 Episode Num: 136 Reward: 628.8008739050431\n",
            "Total Timesteps: 110045 Episode Num: 137 Reward: 724.6351596457329\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 581.465084\n",
            "---------------------------------------\n",
            "Total Timesteps: 111045 Episode Num: 138 Reward: 649.752184731671\n",
            "Total Timesteps: 112045 Episode Num: 139 Reward: 418.80517946025907\n",
            "Total Timesteps: 113045 Episode Num: 140 Reward: 238.43412439901365\n",
            "Total Timesteps: 114045 Episode Num: 141 Reward: 678.6531301541993\n",
            "Total Timesteps: 115045 Episode Num: 142 Reward: 499.27781387617773\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 484.930481\n",
            "---------------------------------------\n",
            "Total Timesteps: 116045 Episode Num: 143 Reward: 433.7795529606454\n",
            "Total Timesteps: 117045 Episode Num: 144 Reward: 533.56082106605\n",
            "Total Timesteps: 118045 Episode Num: 145 Reward: 634.8519201974192\n",
            "Total Timesteps: 119045 Episode Num: 146 Reward: 715.6727077807235\n",
            "Total Timesteps: 120045 Episode Num: 147 Reward: 608.527546221553\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 570.872583\n",
            "---------------------------------------\n",
            "Total Timesteps: 121045 Episode Num: 148 Reward: 647.1748467788998\n",
            "Total Timesteps: 122045 Episode Num: 149 Reward: 533.5614036457811\n",
            "Total Timesteps: 123045 Episode Num: 150 Reward: 515.7074230152933\n",
            "Total Timesteps: 124045 Episode Num: 151 Reward: 542.7906255037068\n",
            "Total Timesteps: 125045 Episode Num: 152 Reward: 743.1224606671523\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 607.223366\n",
            "---------------------------------------\n",
            "Total Timesteps: 126045 Episode Num: 153 Reward: 623.9645908087809\n",
            "Total Timesteps: 127045 Episode Num: 154 Reward: 596.4699299871454\n",
            "Total Timesteps: 128045 Episode Num: 155 Reward: 647.3143424590406\n",
            "Total Timesteps: 129045 Episode Num: 156 Reward: 610.4510004902086\n",
            "Total Timesteps: 130045 Episode Num: 157 Reward: 429.5392481077622\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 591.444315\n",
            "---------------------------------------\n",
            "Total Timesteps: 131045 Episode Num: 158 Reward: 576.4477177251117\n",
            "Total Timesteps: 132045 Episode Num: 159 Reward: 477.7067186859836\n",
            "Total Timesteps: 133045 Episode Num: 160 Reward: 541.1478390271467\n",
            "Total Timesteps: 134045 Episode Num: 161 Reward: 669.5701047911649\n",
            "Total Timesteps: 135045 Episode Num: 162 Reward: 615.6644471065498\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 730.220519\n",
            "---------------------------------------\n",
            "Total Timesteps: 136045 Episode Num: 163 Reward: 474.63759367466935\n",
            "Total Timesteps: 137045 Episode Num: 164 Reward: 597.3974797251625\n",
            "Total Timesteps: 138045 Episode Num: 165 Reward: 567.1243201707828\n",
            "Total Timesteps: 139045 Episode Num: 166 Reward: 435.03223195305185\n",
            "Total Timesteps: 140045 Episode Num: 167 Reward: 677.5037628541418\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 684.262957\n",
            "---------------------------------------\n",
            "Total Timesteps: 141045 Episode Num: 168 Reward: 731.2920416999984\n",
            "Total Timesteps: 142045 Episode Num: 169 Reward: 562.9105020349691\n",
            "Total Timesteps: 143045 Episode Num: 170 Reward: 607.0191442541333\n",
            "Total Timesteps: 144045 Episode Num: 171 Reward: 563.4420947231876\n",
            "Total Timesteps: 145045 Episode Num: 172 Reward: 539.2110836054055\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 581.940092\n",
            "---------------------------------------\n",
            "Total Timesteps: 146045 Episode Num: 173 Reward: 654.6591412535139\n",
            "Total Timesteps: 147045 Episode Num: 174 Reward: 674.2253434015602\n",
            "Total Timesteps: 148045 Episode Num: 175 Reward: 614.9953179357966\n",
            "Total Timesteps: 149045 Episode Num: 176 Reward: 660.0618911141296\n",
            "Total Timesteps: 150045 Episode Num: 177 Reward: 605.266465284663\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 460.426288\n",
            "---------------------------------------\n",
            "Total Timesteps: 151045 Episode Num: 178 Reward: 435.589378883665\n",
            "Total Timesteps: 152045 Episode Num: 179 Reward: 637.5498129439485\n",
            "Total Timesteps: 153045 Episode Num: 180 Reward: 769.4907115435352\n",
            "Total Timesteps: 154045 Episode Num: 181 Reward: 750.4049648245904\n",
            "Total Timesteps: 155045 Episode Num: 182 Reward: 554.655561809052\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 511.240076\n",
            "---------------------------------------\n",
            "Total Timesteps: 156045 Episode Num: 183 Reward: 657.8835228774138\n",
            "Total Timesteps: 157045 Episode Num: 184 Reward: 340.2682529660141\n",
            "Total Timesteps: 158045 Episode Num: 185 Reward: 528.47698772463\n",
            "Total Timesteps: 159045 Episode Num: 186 Reward: 735.8565447031638\n",
            "Total Timesteps: 160045 Episode Num: 187 Reward: 492.766554997329\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 569.388587\n",
            "---------------------------------------\n",
            "Total Timesteps: 161045 Episode Num: 188 Reward: 447.3393587364092\n",
            "Total Timesteps: 162045 Episode Num: 189 Reward: 497.7576852553811\n",
            "Total Timesteps: 163045 Episode Num: 190 Reward: 769.0245877057336\n",
            "Total Timesteps: 164045 Episode Num: 191 Reward: 562.2473230833884\n",
            "Total Timesteps: 165045 Episode Num: 192 Reward: 778.8639529225146\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 677.846581\n",
            "---------------------------------------\n",
            "Total Timesteps: 166045 Episode Num: 193 Reward: 603.3791034951689\n",
            "Total Timesteps: 167045 Episode Num: 194 Reward: 844.4182603667776\n",
            "Total Timesteps: 168045 Episode Num: 195 Reward: 848.9368323885088\n",
            "Total Timesteps: 169045 Episode Num: 196 Reward: 675.2284557941783\n",
            "Total Timesteps: 170045 Episode Num: 197 Reward: 729.3354686572048\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 787.841468\n",
            "---------------------------------------\n",
            "Total Timesteps: 171045 Episode Num: 198 Reward: 893.3894367471225\n",
            "Total Timesteps: 172045 Episode Num: 199 Reward: 721.7227970826636\n",
            "Total Timesteps: 173045 Episode Num: 200 Reward: 737.6511273611162\n",
            "Total Timesteps: 174045 Episode Num: 201 Reward: 825.2066986477795\n",
            "Total Timesteps: 175045 Episode Num: 202 Reward: 475.9234851466543\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 611.483209\n",
            "---------------------------------------\n",
            "Total Timesteps: 176045 Episode Num: 203 Reward: 543.4464109409873\n",
            "Total Timesteps: 177045 Episode Num: 204 Reward: 612.9869257468744\n",
            "Total Timesteps: 178045 Episode Num: 205 Reward: 767.4446345183475\n",
            "Total Timesteps: 179045 Episode Num: 206 Reward: 735.7675592841069\n",
            "Total Timesteps: 180045 Episode Num: 207 Reward: 821.4883692947053\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 722.753014\n",
            "---------------------------------------\n",
            "Total Timesteps: 181045 Episode Num: 208 Reward: 848.8105607241287\n",
            "Total Timesteps: 182045 Episode Num: 209 Reward: 626.5588247866236\n",
            "Total Timesteps: 183045 Episode Num: 210 Reward: 667.3805470373699\n",
            "Total Timesteps: 184045 Episode Num: 211 Reward: 584.338238974056\n",
            "Total Timesteps: 185045 Episode Num: 212 Reward: 710.040525755358\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 727.001891\n",
            "---------------------------------------\n",
            "Total Timesteps: 186045 Episode Num: 213 Reward: 731.7057689792751\n",
            "Total Timesteps: 187045 Episode Num: 214 Reward: 797.8140260571548\n",
            "Total Timesteps: 188045 Episode Num: 215 Reward: 458.9814503169396\n",
            "Total Timesteps: 189045 Episode Num: 216 Reward: 657.6136674162465\n",
            "Total Timesteps: 190045 Episode Num: 217 Reward: 786.6208609832665\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 646.516051\n",
            "---------------------------------------\n",
            "Total Timesteps: 191045 Episode Num: 218 Reward: 729.3367470094317\n",
            "Total Timesteps: 192045 Episode Num: 219 Reward: 545.1171074513092\n",
            "Total Timesteps: 193045 Episode Num: 220 Reward: 603.8652319308641\n",
            "Total Timesteps: 194045 Episode Num: 221 Reward: 686.4420807265304\n",
            "Total Timesteps: 195045 Episode Num: 222 Reward: 736.4271385179195\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 648.323901\n",
            "---------------------------------------\n",
            "Total Timesteps: 196045 Episode Num: 223 Reward: 659.9553951275967\n",
            "Total Timesteps: 197045 Episode Num: 224 Reward: 808.4252920075419\n",
            "Total Timesteps: 198045 Episode Num: 225 Reward: 645.2338027409693\n",
            "Total Timesteps: 199045 Episode Num: 226 Reward: 418.0796735878241\n",
            "Total Timesteps: 200045 Episode Num: 227 Reward: 515.9111859029891\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 735.770465\n",
            "---------------------------------------\n",
            "Total Timesteps: 201045 Episode Num: 228 Reward: 813.8666548452735\n",
            "Total Timesteps: 202045 Episode Num: 229 Reward: 657.5487542481426\n",
            "Total Timesteps: 203045 Episode Num: 230 Reward: 577.7558446875973\n",
            "Total Timesteps: 204045 Episode Num: 231 Reward: 650.0833042104213\n",
            "Total Timesteps: 205045 Episode Num: 232 Reward: 499.17457359704156\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 725.392411\n",
            "---------------------------------------\n",
            "Total Timesteps: 206045 Episode Num: 233 Reward: 524.4150228889225\n",
            "Total Timesteps: 207045 Episode Num: 234 Reward: 482.2848610880955\n",
            "Total Timesteps: 208045 Episode Num: 235 Reward: 648.5672084514969\n",
            "Total Timesteps: 209045 Episode Num: 236 Reward: 531.071309912\n",
            "Total Timesteps: 210045 Episode Num: 237 Reward: 626.0302932812868\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 640.400459\n",
            "---------------------------------------\n",
            "Total Timesteps: 211045 Episode Num: 238 Reward: 629.6960329057002\n",
            "Total Timesteps: 212045 Episode Num: 239 Reward: 737.7760263588743\n",
            "Total Timesteps: 213045 Episode Num: 240 Reward: 815.0612587027705\n",
            "Total Timesteps: 214045 Episode Num: 241 Reward: 737.4829899616703\n",
            "Total Timesteps: 215045 Episode Num: 242 Reward: 388.42818269664997\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 613.477763\n",
            "---------------------------------------\n",
            "Total Timesteps: 216045 Episode Num: 243 Reward: 584.5422760976722\n",
            "Total Timesteps: 217045 Episode Num: 244 Reward: 802.7738493415553\n",
            "Total Timesteps: 218045 Episode Num: 245 Reward: 620.1428066156022\n",
            "Total Timesteps: 219045 Episode Num: 246 Reward: 675.3609377131085\n",
            "Total Timesteps: 220045 Episode Num: 247 Reward: 655.5861517797885\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 755.873512\n",
            "---------------------------------------\n",
            "Total Timesteps: 221045 Episode Num: 248 Reward: 831.6906295566299\n",
            "Total Timesteps: 222045 Episode Num: 249 Reward: 542.7030335779953\n",
            "Total Timesteps: 223045 Episode Num: 250 Reward: 587.2346646076847\n",
            "Total Timesteps: 224045 Episode Num: 251 Reward: 514.5179533225528\n",
            "Total Timesteps: 225045 Episode Num: 252 Reward: 705.1884214769495\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 745.424313\n",
            "---------------------------------------\n",
            "Total Timesteps: 226045 Episode Num: 253 Reward: 556.2422789310168\n",
            "Total Timesteps: 227045 Episode Num: 254 Reward: 891.9645603256209\n",
            "Total Timesteps: 228045 Episode Num: 255 Reward: 992.0456298298024\n",
            "Total Timesteps: 229045 Episode Num: 256 Reward: 857.7113898957556\n",
            "Total Timesteps: 230045 Episode Num: 257 Reward: 864.9111689533954\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 741.475634\n",
            "---------------------------------------\n",
            "Total Timesteps: 231045 Episode Num: 258 Reward: 786.8436535509999\n",
            "Total Timesteps: 232045 Episode Num: 259 Reward: 905.8490710083049\n",
            "Total Timesteps: 233045 Episode Num: 260 Reward: 855.9195276890656\n",
            "Total Timesteps: 234045 Episode Num: 261 Reward: 863.6070690803198\n",
            "Total Timesteps: 235045 Episode Num: 262 Reward: 894.7920266928269\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 863.922404\n",
            "---------------------------------------\n",
            "Total Timesteps: 236045 Episode Num: 263 Reward: 836.3845265918079\n",
            "Total Timesteps: 237045 Episode Num: 264 Reward: 904.0633068644644\n",
            "Total Timesteps: 238045 Episode Num: 265 Reward: 1074.6674255873795\n",
            "Total Timesteps: 239045 Episode Num: 266 Reward: 1004.0061574765799\n",
            "Total Timesteps: 240045 Episode Num: 267 Reward: 978.6293408250002\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 879.921063\n",
            "---------------------------------------\n",
            "Total Timesteps: 241045 Episode Num: 268 Reward: 866.1619519339781\n",
            "Total Timesteps: 242045 Episode Num: 269 Reward: 790.3028737383261\n",
            "Total Timesteps: 243045 Episode Num: 270 Reward: 713.6041838014985\n",
            "Total Timesteps: 244045 Episode Num: 271 Reward: 687.5126521649829\n",
            "Total Timesteps: 245045 Episode Num: 272 Reward: 958.4546553162818\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1041.885170\n",
            "---------------------------------------\n",
            "Total Timesteps: 246045 Episode Num: 273 Reward: 758.2800752039807\n",
            "Total Timesteps: 247045 Episode Num: 274 Reward: 1159.9352988265612\n",
            "Total Timesteps: 248045 Episode Num: 275 Reward: 867.1371551851735\n",
            "Total Timesteps: 249045 Episode Num: 276 Reward: 987.0608806285579\n",
            "Total Timesteps: 250045 Episode Num: 277 Reward: 1017.6387127290267\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 852.834945\n",
            "---------------------------------------\n",
            "Total Timesteps: 251045 Episode Num: 278 Reward: 1052.335925017304\n",
            "Total Timesteps: 252045 Episode Num: 279 Reward: 1260.8670268330588\n",
            "Total Timesteps: 253045 Episode Num: 280 Reward: 1015.7016495159086\n",
            "Total Timesteps: 254045 Episode Num: 281 Reward: 1313.0034343996456\n",
            "Total Timesteps: 255045 Episode Num: 282 Reward: 1256.6643574253678\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1151.994169\n",
            "---------------------------------------\n",
            "Total Timesteps: 256045 Episode Num: 283 Reward: 1256.337575887683\n",
            "Total Timesteps: 257045 Episode Num: 284 Reward: 895.5237297862361\n",
            "Total Timesteps: 258045 Episode Num: 285 Reward: 1153.5690961856333\n",
            "Total Timesteps: 259045 Episode Num: 286 Reward: 1130.2144907348286\n",
            "Total Timesteps: 260045 Episode Num: 287 Reward: 885.2377577313581\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 883.832184\n",
            "---------------------------------------\n",
            "Total Timesteps: 261045 Episode Num: 288 Reward: 950.2140007329726\n",
            "Total Timesteps: 262045 Episode Num: 289 Reward: 768.285086353273\n",
            "Total Timesteps: 263045 Episode Num: 290 Reward: 740.1348032654522\n",
            "Total Timesteps: 264045 Episode Num: 291 Reward: 1184.6037178301842\n",
            "Total Timesteps: 265045 Episode Num: 292 Reward: 1313.5187872836505\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1280.805509\n",
            "---------------------------------------\n",
            "Total Timesteps: 266045 Episode Num: 293 Reward: 1304.7740565182241\n",
            "Total Timesteps: 267045 Episode Num: 294 Reward: 776.5094330021873\n",
            "Total Timesteps: 268045 Episode Num: 295 Reward: 965.2591309727419\n",
            "Total Timesteps: 269045 Episode Num: 296 Reward: 878.6600333873878\n",
            "Total Timesteps: 270045 Episode Num: 297 Reward: 1146.375285726995\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1299.144709\n",
            "---------------------------------------\n",
            "Total Timesteps: 271045 Episode Num: 298 Reward: 1293.4394607326108\n",
            "Total Timesteps: 272045 Episode Num: 299 Reward: 1408.4869944941715\n",
            "Total Timesteps: 273045 Episode Num: 300 Reward: 1453.9989509035702\n",
            "Total Timesteps: 274045 Episode Num: 301 Reward: 1301.9336484246928\n",
            "Total Timesteps: 275045 Episode Num: 302 Reward: 1118.1841330111406\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1321.989591\n",
            "---------------------------------------\n",
            "Total Timesteps: 276045 Episode Num: 303 Reward: 1280.8617947997373\n",
            "Total Timesteps: 277045 Episode Num: 304 Reward: 1285.9038916342652\n",
            "Total Timesteps: 278045 Episode Num: 305 Reward: 1505.0484920556075\n",
            "Total Timesteps: 279045 Episode Num: 306 Reward: 1477.481703001183\n",
            "Total Timesteps: 280045 Episode Num: 307 Reward: 1287.8346193690647\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1552.147062\n",
            "---------------------------------------\n",
            "Total Timesteps: 281045 Episode Num: 308 Reward: 1463.2209726670217\n",
            "Total Timesteps: 282045 Episode Num: 309 Reward: 1536.5387726817303\n",
            "Total Timesteps: 283045 Episode Num: 310 Reward: 1414.7371285494346\n",
            "Total Timesteps: 284045 Episode Num: 311 Reward: 1472.9061950704447\n",
            "Total Timesteps: 285045 Episode Num: 312 Reward: 1538.2312785700947\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1395.980164\n",
            "---------------------------------------\n",
            "Total Timesteps: 286045 Episode Num: 313 Reward: 1519.6905929350353\n",
            "Total Timesteps: 287045 Episode Num: 314 Reward: 1656.9829415739807\n",
            "Total Timesteps: 288045 Episode Num: 315 Reward: 1369.207156107834\n",
            "Total Timesteps: 289045 Episode Num: 316 Reward: 1682.3932887220597\n",
            "Total Timesteps: 290045 Episode Num: 317 Reward: 1543.3370393930275\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1676.806195\n",
            "---------------------------------------\n",
            "Total Timesteps: 291045 Episode Num: 318 Reward: 1627.7264624405361\n",
            "Total Timesteps: 292045 Episode Num: 319 Reward: 1438.8760917606571\n",
            "Total Timesteps: 293045 Episode Num: 320 Reward: 1568.8019782993895\n",
            "Total Timesteps: 294045 Episode Num: 321 Reward: 1408.0298189371358\n",
            "Total Timesteps: 295045 Episode Num: 322 Reward: 1718.442775354241\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1584.292478\n",
            "---------------------------------------\n",
            "Total Timesteps: 296045 Episode Num: 323 Reward: 1516.1503189647067\n",
            "Total Timesteps: 297045 Episode Num: 324 Reward: 1338.0107747249658\n",
            "Total Timesteps: 298045 Episode Num: 325 Reward: 1583.7418209282455\n",
            "Total Timesteps: 299045 Episode Num: 326 Reward: 1724.8880755175153\n",
            "Total Timesteps: 300045 Episode Num: 327 Reward: 1598.7218076894462\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1637.785626\n",
            "---------------------------------------\n",
            "Total Timesteps: 301045 Episode Num: 328 Reward: 1628.4122060415373\n",
            "Total Timesteps: 302045 Episode Num: 329 Reward: 1493.292778201784\n",
            "Total Timesteps: 303045 Episode Num: 330 Reward: 1742.7048191873266\n",
            "Total Timesteps: 304045 Episode Num: 331 Reward: 1775.3718770980045\n",
            "Total Timesteps: 305045 Episode Num: 332 Reward: 1778.9703649861767\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1737.459342\n",
            "---------------------------------------\n",
            "Total Timesteps: 306045 Episode Num: 333 Reward: 1725.0117308950428\n",
            "Total Timesteps: 307045 Episode Num: 334 Reward: 1688.997621714954\n",
            "Total Timesteps: 308045 Episode Num: 335 Reward: 868.3054765505339\n",
            "Total Timesteps: 309045 Episode Num: 336 Reward: 1553.0509609392386\n",
            "Total Timesteps: 310045 Episode Num: 337 Reward: 1704.9188137210365\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1434.405192\n",
            "---------------------------------------\n",
            "Total Timesteps: 311045 Episode Num: 338 Reward: 926.3743242848988\n",
            "Total Timesteps: 312045 Episode Num: 339 Reward: 1459.997117558269\n",
            "Total Timesteps: 313045 Episode Num: 340 Reward: 1764.4743832218303\n",
            "Total Timesteps: 314045 Episode Num: 341 Reward: 1788.7926149437271\n",
            "Total Timesteps: 315045 Episode Num: 342 Reward: 1735.603807640869\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1864.792148\n",
            "---------------------------------------\n",
            "Total Timesteps: 316045 Episode Num: 343 Reward: 1899.2412287095572\n",
            "Total Timesteps: 317045 Episode Num: 344 Reward: 1751.3276676010535\n",
            "Total Timesteps: 318045 Episode Num: 345 Reward: 1765.404013167548\n",
            "Total Timesteps: 319045 Episode Num: 346 Reward: 1635.0305236396537\n",
            "Total Timesteps: 320045 Episode Num: 347 Reward: 1724.1221845616915\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1654.676281\n",
            "---------------------------------------\n",
            "Total Timesteps: 321045 Episode Num: 348 Reward: 1607.7059538492156\n",
            "Total Timesteps: 322045 Episode Num: 349 Reward: 1691.476108608391\n",
            "Total Timesteps: 323045 Episode Num: 350 Reward: 1739.5505509627917\n",
            "Total Timesteps: 324045 Episode Num: 351 Reward: 1755.51563149355\n",
            "Total Timesteps: 325045 Episode Num: 352 Reward: 1731.3495915307662\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1746.370816\n",
            "---------------------------------------\n",
            "Total Timesteps: 326045 Episode Num: 353 Reward: 1698.803653052945\n",
            "Total Timesteps: 327045 Episode Num: 354 Reward: 1792.3436373179456\n",
            "Total Timesteps: 328045 Episode Num: 355 Reward: 1901.871339051801\n",
            "Total Timesteps: 329045 Episode Num: 356 Reward: 1728.040223655907\n",
            "Total Timesteps: 330045 Episode Num: 357 Reward: 1852.6815602873514\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1847.902860\n",
            "---------------------------------------\n",
            "Total Timesteps: 331045 Episode Num: 358 Reward: 1816.2955047907788\n",
            "Total Timesteps: 332045 Episode Num: 359 Reward: 1891.8730241914513\n",
            "Total Timesteps: 333045 Episode Num: 360 Reward: 1951.4610169773964\n",
            "Total Timesteps: 334045 Episode Num: 361 Reward: 1738.4386106526385\n",
            "Total Timesteps: 335045 Episode Num: 362 Reward: 1823.9997205772852\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1899.392969\n",
            "---------------------------------------\n",
            "Total Timesteps: 336045 Episode Num: 363 Reward: 1872.127294232938\n",
            "Total Timesteps: 337045 Episode Num: 364 Reward: 1858.8347639744113\n",
            "Total Timesteps: 338045 Episode Num: 365 Reward: 1938.157972836611\n",
            "Total Timesteps: 339045 Episode Num: 366 Reward: 1879.6510027369761\n",
            "Total Timesteps: 340045 Episode Num: 367 Reward: 1732.8316190304324\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1997.542340\n",
            "---------------------------------------\n",
            "Total Timesteps: 341045 Episode Num: 368 Reward: 2032.9355176929248\n",
            "Total Timesteps: 342045 Episode Num: 369 Reward: 1837.2719210589923\n",
            "Total Timesteps: 343045 Episode Num: 370 Reward: 1995.2676629366401\n",
            "Total Timesteps: 344045 Episode Num: 371 Reward: 1984.1717340852272\n",
            "Total Timesteps: 345045 Episode Num: 372 Reward: 1995.7840914029987\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1938.719993\n",
            "---------------------------------------\n",
            "Total Timesteps: 346045 Episode Num: 373 Reward: 1897.8273783860352\n",
            "Total Timesteps: 347045 Episode Num: 374 Reward: 2003.4135887477019\n",
            "Total Timesteps: 348045 Episode Num: 375 Reward: 2047.3714190949304\n",
            "Total Timesteps: 349045 Episode Num: 376 Reward: 2059.667528001469\n",
            "Total Timesteps: 350045 Episode Num: 377 Reward: 2023.8118339023633\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2066.275802\n",
            "---------------------------------------\n",
            "Total Timesteps: 351045 Episode Num: 378 Reward: 2034.3170383066715\n",
            "Total Timesteps: 352045 Episode Num: 379 Reward: 2057.724749661316\n",
            "Total Timesteps: 353045 Episode Num: 380 Reward: 1905.7121808874774\n",
            "Total Timesteps: 354045 Episode Num: 381 Reward: 1897.0253061898704\n",
            "Total Timesteps: 355045 Episode Num: 382 Reward: 2014.753904101264\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2039.721688\n",
            "---------------------------------------\n",
            "Total Timesteps: 356045 Episode Num: 383 Reward: 2000.6018444231474\n",
            "Total Timesteps: 357045 Episode Num: 384 Reward: 2007.7953281568944\n",
            "Total Timesteps: 358045 Episode Num: 385 Reward: 1986.1877631959542\n",
            "Total Timesteps: 359045 Episode Num: 386 Reward: 2002.9456748816835\n",
            "Total Timesteps: 360045 Episode Num: 387 Reward: 2019.8970525139882\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2139.548496\n",
            "---------------------------------------\n",
            "Total Timesteps: 361045 Episode Num: 388 Reward: 2126.801597041985\n",
            "Total Timesteps: 362045 Episode Num: 389 Reward: 2171.00077422665\n",
            "Total Timesteps: 363045 Episode Num: 390 Reward: 2078.906728667533\n",
            "Total Timesteps: 364045 Episode Num: 391 Reward: 1988.707367817982\n",
            "Total Timesteps: 365045 Episode Num: 392 Reward: 1987.773322840964\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2101.126246\n",
            "---------------------------------------\n",
            "Total Timesteps: 366045 Episode Num: 393 Reward: 2063.1907166186725\n",
            "Total Timesteps: 367045 Episode Num: 394 Reward: 2129.8578247388364\n",
            "Total Timesteps: 368045 Episode Num: 395 Reward: 2020.0368656000367\n",
            "Total Timesteps: 369045 Episode Num: 396 Reward: 2062.5248508820796\n",
            "Total Timesteps: 370045 Episode Num: 397 Reward: 2014.93374310722\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2099.633124\n",
            "---------------------------------------\n",
            "Total Timesteps: 371045 Episode Num: 398 Reward: 2128.1662393154265\n",
            "Total Timesteps: 372045 Episode Num: 399 Reward: 1858.975313679885\n",
            "Total Timesteps: 373045 Episode Num: 400 Reward: 2202.764991810617\n",
            "Total Timesteps: 374045 Episode Num: 401 Reward: 2191.957677724085\n",
            "Total Timesteps: 375045 Episode Num: 402 Reward: 2214.120599380938\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2218.722518\n",
            "---------------------------------------\n",
            "Total Timesteps: 376045 Episode Num: 403 Reward: 2171.4372974357952\n",
            "Total Timesteps: 377045 Episode Num: 404 Reward: 2037.13710902862\n",
            "Total Timesteps: 378045 Episode Num: 405 Reward: 2199.832033599828\n",
            "Total Timesteps: 379045 Episode Num: 406 Reward: 2220.2549397834823\n",
            "Total Timesteps: 380045 Episode Num: 407 Reward: 2092.579652714562\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2211.662840\n",
            "---------------------------------------\n",
            "Total Timesteps: 381045 Episode Num: 408 Reward: 2186.547844089493\n",
            "Total Timesteps: 382045 Episode Num: 409 Reward: 2169.1416038711805\n",
            "Total Timesteps: 383045 Episode Num: 410 Reward: 2206.463154679975\n",
            "Total Timesteps: 384045 Episode Num: 411 Reward: 2081.3754330768056\n",
            "Total Timesteps: 385045 Episode Num: 412 Reward: 2185.5899998406976\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2170.494432\n",
            "---------------------------------------\n",
            "Total Timesteps: 386045 Episode Num: 413 Reward: 2151.987339593037\n",
            "Total Timesteps: 387045 Episode Num: 414 Reward: 2252.5389351837116\n",
            "Total Timesteps: 388045 Episode Num: 415 Reward: 2163.366241196944\n",
            "Total Timesteps: 389045 Episode Num: 416 Reward: 2269.211911308802\n",
            "Total Timesteps: 390045 Episode Num: 417 Reward: 2185.150867469918\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2169.376809\n",
            "---------------------------------------\n",
            "Total Timesteps: 391045 Episode Num: 418 Reward: 2092.5437239746684\n",
            "Total Timesteps: 392045 Episode Num: 419 Reward: 2188.4580354539053\n",
            "Total Timesteps: 393045 Episode Num: 420 Reward: 2222.4722953472256\n",
            "Total Timesteps: 394045 Episode Num: 421 Reward: 2236.3384505741983\n",
            "Total Timesteps: 395045 Episode Num: 422 Reward: 2223.9593549561164\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2317.805430\n",
            "---------------------------------------\n",
            "Total Timesteps: 396045 Episode Num: 423 Reward: 2195.47280455091\n",
            "Total Timesteps: 397045 Episode Num: 424 Reward: 2258.1057148269415\n",
            "Total Timesteps: 398045 Episode Num: 425 Reward: 2207.0071357961947\n",
            "Total Timesteps: 399045 Episode Num: 426 Reward: 2228.34388395844\n",
            "Total Timesteps: 400045 Episode Num: 427 Reward: 2267.1632605266336\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2267.738119\n",
            "---------------------------------------\n",
            "Total Timesteps: 401045 Episode Num: 428 Reward: 2233.4801026329615\n",
            "Total Timesteps: 402045 Episode Num: 429 Reward: 2210.6622964486232\n",
            "Total Timesteps: 403045 Episode Num: 430 Reward: 2250.589926854289\n",
            "Total Timesteps: 404045 Episode Num: 431 Reward: 2247.655028139296\n",
            "Total Timesteps: 405045 Episode Num: 432 Reward: 2368.7643041416227\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2281.634484\n",
            "---------------------------------------\n",
            "Total Timesteps: 406045 Episode Num: 433 Reward: 2224.3759673065433\n",
            "Total Timesteps: 407045 Episode Num: 434 Reward: 2297.516623967229\n",
            "Total Timesteps: 408045 Episode Num: 435 Reward: 2289.627440687152\n",
            "Total Timesteps: 409045 Episode Num: 436 Reward: 2301.692828467435\n",
            "Total Timesteps: 410045 Episode Num: 437 Reward: 2301.4912185588687\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2322.048263\n",
            "---------------------------------------\n",
            "Total Timesteps: 411045 Episode Num: 438 Reward: 2254.1724100564734\n",
            "Total Timesteps: 412045 Episode Num: 439 Reward: 2257.867008182498\n",
            "Total Timesteps: 413045 Episode Num: 440 Reward: 2247.581655070034\n",
            "Total Timesteps: 414045 Episode Num: 441 Reward: 2232.254659715105\n",
            "Total Timesteps: 415045 Episode Num: 442 Reward: 2179.67942689241\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1670.146197\n",
            "---------------------------------------\n",
            "Total Timesteps: 416045 Episode Num: 443 Reward: 2159.560546508838\n",
            "Total Timesteps: 417045 Episode Num: 444 Reward: 2224.881771227651\n",
            "Total Timesteps: 418045 Episode Num: 445 Reward: 207.84240290845747\n",
            "Total Timesteps: 419045 Episode Num: 446 Reward: 180.7749459766125\n",
            "Total Timesteps: 420045 Episode Num: 447 Reward: 371.79076787740144\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 260.828099\n",
            "---------------------------------------\n",
            "Total Timesteps: 421045 Episode Num: 448 Reward: 203.447710827629\n",
            "Total Timesteps: 422045 Episode Num: 449 Reward: 297.3068833612892\n",
            "Total Timesteps: 423045 Episode Num: 450 Reward: 302.49300885399117\n",
            "Total Timesteps: 424045 Episode Num: 451 Reward: 2246.9373596067794\n",
            "Total Timesteps: 425045 Episode Num: 452 Reward: 320.5214776591316\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2247.220827\n",
            "---------------------------------------\n",
            "Total Timesteps: 426045 Episode Num: 453 Reward: 2220.7330504284064\n",
            "Total Timesteps: 427045 Episode Num: 454 Reward: 2303.6724205299893\n",
            "Total Timesteps: 428045 Episode Num: 455 Reward: 2264.547198293776\n",
            "Total Timesteps: 429045 Episode Num: 456 Reward: 2442.567477429906\n",
            "Total Timesteps: 430045 Episode Num: 457 Reward: 2359.8197360995505\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2278.596283\n",
            "---------------------------------------\n",
            "Total Timesteps: 431045 Episode Num: 458 Reward: 2196.9738893826734\n",
            "Total Timesteps: 432045 Episode Num: 459 Reward: 2314.0503601590794\n",
            "Total Timesteps: 433045 Episode Num: 460 Reward: 2323.4420846314297\n",
            "Total Timesteps: 434045 Episode Num: 461 Reward: 2414.7926065406336\n",
            "Total Timesteps: 435045 Episode Num: 462 Reward: 2350.535718174458\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2419.169500\n",
            "---------------------------------------\n",
            "Total Timesteps: 436045 Episode Num: 463 Reward: 2345.2019126832415\n",
            "Total Timesteps: 437045 Episode Num: 464 Reward: 2362.803481647136\n",
            "Total Timesteps: 438045 Episode Num: 465 Reward: 2359.8823168592303\n",
            "Total Timesteps: 439045 Episode Num: 466 Reward: 2215.9052604215904\n",
            "Total Timesteps: 440045 Episode Num: 467 Reward: 2354.165647233671\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2507.157066\n",
            "---------------------------------------\n",
            "Total Timesteps: 441045 Episode Num: 468 Reward: 2437.1213285681383\n",
            "Total Timesteps: 442045 Episode Num: 469 Reward: 2391.9517931689825\n",
            "Total Timesteps: 443045 Episode Num: 470 Reward: 2353.4495997631816\n",
            "Total Timesteps: 444045 Episode Num: 471 Reward: 2472.126370687931\n",
            "Total Timesteps: 445045 Episode Num: 472 Reward: 2420.3334116733567\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2538.296080\n",
            "---------------------------------------\n",
            "Total Timesteps: 446045 Episode Num: 473 Reward: 2503.3629300917382\n",
            "Total Timesteps: 447045 Episode Num: 474 Reward: 2413.4909028189127\n",
            "Total Timesteps: 448045 Episode Num: 475 Reward: 2338.713630807336\n",
            "Total Timesteps: 449045 Episode Num: 476 Reward: 2383.2499784682373\n",
            "Total Timesteps: 450045 Episode Num: 477 Reward: 2341.662406018459\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2397.923982\n",
            "---------------------------------------\n",
            "Total Timesteps: 451045 Episode Num: 478 Reward: 2407.438453440144\n",
            "Total Timesteps: 452045 Episode Num: 479 Reward: 2244.0660192774753\n",
            "Total Timesteps: 453045 Episode Num: 480 Reward: 2526.7565129810146\n",
            "Total Timesteps: 454045 Episode Num: 481 Reward: 2439.6521444853884\n",
            "Total Timesteps: 455045 Episode Num: 482 Reward: 2405.978971575217\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2487.992057\n",
            "---------------------------------------\n",
            "Total Timesteps: 456045 Episode Num: 483 Reward: 2423.465798967386\n",
            "Total Timesteps: 457045 Episode Num: 484 Reward: 2263.3331690384903\n",
            "Total Timesteps: 458045 Episode Num: 485 Reward: 2357.6705402576804\n",
            "Total Timesteps: 459045 Episode Num: 486 Reward: 2136.6758350395735\n",
            "Total Timesteps: 460045 Episode Num: 487 Reward: 2310.909698003489\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2293.723255\n",
            "---------------------------------------\n",
            "Total Timesteps: 461045 Episode Num: 488 Reward: 2282.8246567557967\n",
            "Total Timesteps: 462045 Episode Num: 489 Reward: 2369.279574040199\n",
            "Total Timesteps: 463045 Episode Num: 490 Reward: 2523.6124199451774\n",
            "Total Timesteps: 464045 Episode Num: 491 Reward: 2408.8064587132635\n",
            "Total Timesteps: 465045 Episode Num: 492 Reward: 2320.390759824135\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2471.005425\n",
            "---------------------------------------\n",
            "Total Timesteps: 466045 Episode Num: 493 Reward: 2452.810101911981\n",
            "Total Timesteps: 467045 Episode Num: 494 Reward: 2385.413545795296\n",
            "Total Timesteps: 468045 Episode Num: 495 Reward: 2359.357116487873\n",
            "Total Timesteps: 469045 Episode Num: 496 Reward: 2466.590091351492\n",
            "Total Timesteps: 470045 Episode Num: 497 Reward: 2279.484879974114\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2347.369644\n",
            "---------------------------------------\n",
            "Total Timesteps: 471045 Episode Num: 498 Reward: 2308.461009808593\n",
            "Total Timesteps: 472045 Episode Num: 499 Reward: 2477.9094656736365\n",
            "Total Timesteps: 473045 Episode Num: 500 Reward: 2248.0904967085958\n",
            "Total Timesteps: 474045 Episode Num: 501 Reward: 2357.1104141344176\n",
            "Total Timesteps: 475045 Episode Num: 502 Reward: 2515.851813665651\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2517.470985\n",
            "---------------------------------------\n",
            "Total Timesteps: 476045 Episode Num: 503 Reward: 2497.0913145324203\n",
            "Total Timesteps: 477045 Episode Num: 504 Reward: 2396.2343454714346\n",
            "Total Timesteps: 478045 Episode Num: 505 Reward: 2430.412396754827\n",
            "Total Timesteps: 479045 Episode Num: 506 Reward: 2434.5555895294237\n",
            "Total Timesteps: 480045 Episode Num: 507 Reward: 2324.786990581662\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2472.073138\n",
            "---------------------------------------\n",
            "Total Timesteps: 481045 Episode Num: 508 Reward: 2478.719093369773\n",
            "Total Timesteps: 482045 Episode Num: 509 Reward: 2450.312944395025\n",
            "Total Timesteps: 483045 Episode Num: 510 Reward: 2441.7767525174395\n",
            "Total Timesteps: 484045 Episode Num: 511 Reward: 2488.1702239901897\n",
            "Total Timesteps: 485045 Episode Num: 512 Reward: 2482.92190657008\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2273.415776\n",
            "---------------------------------------\n",
            "Total Timesteps: 486045 Episode Num: 513 Reward: 2326.6144469514525\n",
            "Total Timesteps: 487045 Episode Num: 514 Reward: 2273.694148320886\n",
            "Total Timesteps: 488045 Episode Num: 515 Reward: 2422.3975525285155\n",
            "Total Timesteps: 489045 Episode Num: 516 Reward: 2374.4282689642037\n",
            "Total Timesteps: 490045 Episode Num: 517 Reward: 2432.2278313893444\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2488.036289\n",
            "---------------------------------------\n",
            "Total Timesteps: 491045 Episode Num: 518 Reward: 2503.50472140067\n",
            "Total Timesteps: 492045 Episode Num: 519 Reward: 2466.3358114716184\n",
            "Total Timesteps: 493045 Episode Num: 520 Reward: 2568.3783334236387\n",
            "Total Timesteps: 494045 Episode Num: 521 Reward: 2546.0854918576633\n",
            "Total Timesteps: 495045 Episode Num: 522 Reward: 2492.240774172696\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2493.770999\n",
            "---------------------------------------\n",
            "Total Timesteps: 496045 Episode Num: 523 Reward: 2391.7090164557612\n",
            "Total Timesteps: 497045 Episode Num: 524 Reward: 2333.8738737554568\n",
            "Total Timesteps: 498045 Episode Num: 525 Reward: 2588.0319417553983\n",
            "Total Timesteps: 499045 Episode Num: 526 Reward: 2604.888302912343\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2546.172291\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "c8171524-c784-49cb-8445-4dbceeac3297"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2551.630872\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcnexWrW4a8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}