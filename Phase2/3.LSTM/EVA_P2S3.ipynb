{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA P2S3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkdivya/EVA/blob/master/Phase2/3.LSTM/EVA_P2S3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jofyc9OC4Qcf",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahBVnrNc3E0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQSAaIz4SkA",
        "colab_type": "text"
      },
      "source": [
        "# Read and process data. \n",
        "\n",
        "Download the file from this URL: https://drive.google.com/file/d/1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApqvYUKfWA8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "149cfab7-e729-4847-c9c1-91bdd282ce39"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOGxPDP3Wpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open('/content/drive/My Drive/EVA/NLP/LSTM/text.txt', 'r').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXXMLRb4kXb",
        "colab_type": "text"
      },
      "source": [
        "Process data and calculate indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5TKeiOp4jtl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae34215f-32f2-472e-bd76-64727bdf6f73"
      },
      "source": [
        "chars = list(set(data))\n",
        "data_size, X_size = len(data), len(chars)\n",
        "print(\"Corona Virus article has %d characters, %d unique characters\" %(data_size, X_size))\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corona Virus article has 10223 characters, 75 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C53MB135LRY",
        "colab_type": "text"
      },
      "source": [
        "# Constants and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfj21ORa49Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hidden_Layer_size = 10 #size of the hidden layer\n",
        "#Time_steps = 10 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning Rate\n",
        "weight_sd = 0.1 #Standard deviation of weights for initialization\n",
        "z_size = Hidden_Layer_size + X_size #Size of concatenation(H, X) vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdmJf4Du5uhb",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions and Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seGHei_D5FGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x): # sigmoid function\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def dsigmoid(y): # derivative of sigmoid function\n",
        "  return y * (1 - y)\n",
        "\n",
        "def tanh(x): # tanh function\n",
        "  return np.tanh(x)\n",
        "\n",
        "def dtanh(y): # derivative of tanh\n",
        "  return 1 - y * y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeCvVH1v6Me-",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 1\n",
        "\n",
        "What is the value of sigmoid(0) calculated from  your code? (Answer up to 1 decimal point, e.g. 4.2 and NOT 4.29999999, no rounding off).\n",
        "\n",
        "# Quiz Question 2\n",
        "\n",
        "What is the value of dsigmoid(sigmoid(0)) calculated from your code?? (Answer up to 2 decimal point, e.g. 4.29 and NOT 4.29999999, no rounding off). \n",
        "\n",
        "# Quiz Question 3\n",
        "\n",
        "What is the value of tanh(dsigmoid(sigmoid(0))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off).\n",
        "\n",
        "# Quiz Question 4\n",
        "\n",
        "What is the value of dtanh(tanh(dsigmoid(sigmoid(0)))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv-vrjPEdJZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "1c7befa2-1717-410a-aac8-19ec3cce40dc"
      },
      "source": [
        "print((\"Quiz1 {0:.1f}\".format(sigmoid(0))))\n",
        "print((\"Quiz2 {0:.2f}\".format(dsigmoid(sigmoid(0)))))\n",
        "print((\"Quiz3 {0:.5f}\".format(tanh(dsigmoid(sigmoid(0))))))\n",
        "print((\"Quiz4 {0:.5f}\".format(dtanh(tanh(dsigmoid(sigmoid(0)))))))\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quiz1 0.5\n",
            "Quiz2 0.25\n",
            "Quiz3 0.24492\n",
            "Quiz4 0.94001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeSVipDu8iKE",
        "colab_type": "text"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICbWNemE6LGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "      self.name = name\n",
        "      self.v = value # parameter value\n",
        "      self.d = np.zeros_like(value) # derivative\n",
        "      self.m = np.zeros_like(value) # momentum for Adagrad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j83pZNPE8212",
        "colab_type": "text"
      },
      "source": [
        "We use random weights with normal distribution (0, weight_sd) for  tanh  activation function and (0.5, weight_sd) for  `sigmoid`  activation function.\n",
        "\n",
        "Biases are initialized to zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swHwLXOI9E7V",
        "colab_type": "text"
      },
      "source": [
        "# LSTM \n",
        "You are making this network, please note f, i, c and o (also \"v\") in the image below:\n",
        "![alt text](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
        "\n",
        "Please note that we are concatenating the old_hidden_vector and new_input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0DBzNY-90s5",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 4\n",
        "\n",
        "In the class definition below, what should be size_a, size_b, and size_c? ONLY use the variables defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFuHhqVq6Wge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size_a = Hidden_Layer_size #Hidden Layers size \n",
        "size_b =  z_size # Size of concatenate(H, X) vector\n",
        "size_c = X_size # Number of characters\n",
        "\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C', np.random.randn(size_a, size_b) * weight_sd)\n",
        "        self.b_C = Param('b_C', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o', np.zeros((size_a, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v', np.random.randn(X_size, size_a) * weight_sd)\n",
        "        self.b_v = Param('b_v', np.zeros((size_c, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzmfGLZt_xVs",
        "colab_type": "text"
      },
      "source": [
        "Look at these operations which we'll be writing:\n",
        "\n",
        "**Concatenation of h and x:**\n",
        "\n",
        "$z\\:=\\:\\left[h_{t-1},\\:x\\right]$\n",
        "\n",
        "$f_t=\\sigma\\left(W_f\\cdot z\\:+\\:b_f\\:\\right)$\n",
        "\n",
        "$i_i=\\sigma\\left(W_i\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$\\overline{C_t}=\\tanh\\left(W_C\\cdot z\\:+\\:b_C\\right)$\n",
        "\n",
        "$C_t=f_t\\ast C_{t-1}+i_t\\ast \\overline{C}_t$\n",
        "\n",
        "$o_t=\\sigma\\left(W_o\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$h_t=o_t\\ast\\tanh\\left(C_t\\right)$\n",
        "\n",
        "**Logits:**\n",
        "\n",
        "$v_t=W_v\\cdot h_t+b_v$\n",
        "\n",
        "**Softmax:**\n",
        "\n",
        "$\\hat{y}=softmax\\left(v_t\\right)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUkseNnDott",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (Hidden_Layer_size, 1)\n",
        "    assert C_prev.shape == (Hidden_Layer_size, 1)\n",
        "    #z=[ht−1,x] \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    #ft=σ(Wf⋅z+bf)\n",
        "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
        "    #i=σ(Wi⋅z+bi)\n",
        "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
        "    #C=tanh(WC⋅z+bC)\n",
        "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
        "    C = f * C_prev + i * C_bar\n",
        "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
        "    h = o * tanh(C)\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZrDhZIjFpdI",
        "colab_type": "text"
      },
      "source": [
        "You must finish the function above before you can attempt the questions below. \n",
        "\n",
        "# Quiz Question 5\n",
        "\n",
        "What is the output of 'print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K1yZleTbFGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fb01e39-52a4-4227-fab7-c702e8b67a3d"
      },
      "source": [
        "print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV-YVl_GGiX8",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 6. \n",
        "\n",
        "Assuming you have fixed the forward function, run this command: \n",
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))\n",
        "\n",
        "Now, find these values:\n",
        "\n",
        "\n",
        "1.   print(z.shape)\n",
        "2.   print(np.sum(z))\n",
        "3.   print(np.sum(f))\n",
        "\n",
        "Copy and paste exact values you get in the logs into the quiz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GvKVWmTDt3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKw6PWiEeO1t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "df3c3b94-f99b-4827-c29e-345ead4ac858"
      },
      "source": [
        "print(z.shape)\n",
        "print(np.sum(z))\n",
        "print(np.sum(f))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(175, 1)\n",
            "0.0\n",
            "50.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeSvhkqwILsG",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "Here we are defining the backpropagation. It's too complicated, here is the whole code. (Please note that this would work only if your earlier code is perfect)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIa1jUZiGPmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + Hidden_Layer_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (Hidden_Layer_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:Hidden_Layer_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnc7WpRkIU5S",
        "colab_type": "text"
      },
      "source": [
        "# Forward and Backward Combined Pass\n",
        "\n",
        "Let's first clear the gradients before each backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWoC3U1ITf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XN93UnjIgmA",
        "colab_type": "text"
      },
      "source": [
        "Clip gradients to mitigate exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTsublxIfFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7XUpDTWIl_Y",
        "colab_type": "text"
      },
      "source": [
        "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
        "\n",
        "input, target are list of integers, with character indexes.\n",
        "h_prev is the array of initial h at  h−1  (size H x 1)\n",
        "C_prev is the array of initial C at  C−1  (size H x 1)\n",
        "Returns loss, final  hT  and  CT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNxjTuZIia_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == Time_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcy5u_vRItkV",
        "colab_type": "text"
      },
      "source": [
        "# Sample the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SrtJiwIsSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiWFaWLNIx_L",
        "colab_type": "text"
      },
      "source": [
        "# Training (Adagrad)\n",
        "\n",
        "Update the graph and display a sample output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQYU-7AIw0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXcASJuI73a",
        "colab_type": "text"
      },
      "source": [
        "# Update Parameters\n",
        "\n",
        "\\begin{align}\n",
        "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
        "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR08TvcjI4Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La9vyJ6RJLFK",
        "colab_type": "text"
      },
      "source": [
        "To delay the keyboard interrupt to prevent the training from stopping in the middle of an iteration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDHbMb7JNGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exponential average of loss\n",
        "# Initialize to a error of a random model\n",
        "smooth_loss = -np.log(1.0 / X_size) * Time_steps\n",
        "\n",
        "iteration, pointer = 0, 0\n",
        "\n",
        "# For the graph\n",
        "plot_iter = np.zeros((0))\n",
        "plot_loss = np.zeros((0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF6vS0VWJqsS",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQyNSL0iJOxH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "23470755-ec57-4af5-d76e-e593308f9744"
      },
      "source": [
        "iter = 50000\n",
        "while iter > 0:\n",
        "  # Reset\n",
        "  if pointer + Time_steps >= len(data) or iteration == 0:\n",
        "      g_h_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      g_C_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      pointer = 0\n",
        "\n",
        "\n",
        "  inputs = ([char_to_idx[ch] \n",
        "              for ch in data[pointer: pointer + Time_steps]])\n",
        "  targets = ([char_to_idx[ch] \n",
        "              for ch in data[pointer + 1: pointer + Time_steps + 1]])\n",
        "\n",
        "  loss, g_h_prev, g_C_prev = \\\n",
        "      forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # Print every hundred steps\n",
        "  if iteration % 100 == 0:\n",
        "      update_status(inputs, g_h_prev, g_C_prev)\n",
        "\n",
        "  update_paramters()\n",
        "\n",
        "  plot_iter = np.append(plot_iter, [iteration])\n",
        "  plot_loss = np.append(plot_loss, [loss])\n",
        "\n",
        "  pointer += Time_steps\n",
        "  iteration += 1\n",
        "  iter = iter -1"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD5CAYAAADREwWlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " sow the screener and byongor, or diffese raporneal beplres in the simulith fiblny in a sise hapention and kinstor, and menticing in llkidapoon, Taiwan cDlon., nover nots, aur peopinks and Scunowan a y \n",
            "----\n",
            "iter 49900, loss 17.723046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AKpa1BGOItQ",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 7. \n",
        "\n",
        "Run the above code for 50000 iterations making sure that you have 100 hidden layers and time_steps is 40. What is the loss value you're seeing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEokctzweu68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Hidden_Layer_size = 100 #size of the hidden layer\n",
        "Time_steps = 40 # Number of time steps (length of the sequence) used for training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyHHd_Q2exZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}