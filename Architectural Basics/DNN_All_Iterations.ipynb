{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArchitecturalBasics_DNN_AllIterations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkdivya/EVA/blob/master/Architectural%20Basics/DNN_All_Iterations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Architectural Basics**\n",
        "##**Building a deep Neural network**\n",
        "\n",
        "In this notebook, let's see how to build a neural network from scratch for MNIST image classification and how we can tune our architecture to optimize our network better step by step.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvXkwJutuOYj",
        "colab_type": "text"
      },
      "source": [
        "**Import Libraries and Modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "48a72db7-e801-4c72-ae9f-a4736d0f24a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "# Importing Keras library\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjlTGDq8O2LP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# Importing Keras classes used for building CNN\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "# Importing Keras Inbuilt dataset\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "outputId": "03d95f08-b590-4429-cc82-9a07b6607f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "c34e4354-07d7-473b-c1a3-3fd0f2a27282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "#Visualize one image in training set\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f11b03c8eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBX30JkCLaOt",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping to hold the no of channels\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data converted to float and performed Image standardization\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "c6a8e298-7143-46f3-c5e2-a4649c893d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Train Labels\n",
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu01vby64HNv",
        "colab_type": "text"
      },
      "source": [
        "**One Hot Encoding**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "a79e7c97-fcce-476f-f8d8-a35b6eb9014d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Train Labels after one hot encoding, for example 5 is converted as [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]\n",
        "Y_train[:10]\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_6PykdOstzq",
        "colab_type": "text"
      },
      "source": [
        "#**Iteration 1**\n",
        "##**How we define a vanila neural network for our classification problem**\n",
        "\n",
        "**Convolution Blocks & Transition Blocks**\n",
        "*   We will start building our network by deciding how many convolution blocks we need based on our input image size and the size of the object we are trying to predict in the image\n",
        "*   And we will decide where we will place our transition blocks (Max pooling and 1x1 block) in our architecture\n",
        "*   We will have convolution blocks followed by transition blocks till we reach our recptive field. \n",
        "\n",
        "**Kernels**\n",
        "*   We will decide the number of kernels used \n",
        "*   We keep increasing our no of kernels until we perform max pooling followed by pointwise convolution\n",
        "\n",
        "**Max Pooling**\n",
        "*   We make sure max pooling is 2 or more layers away from output\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1lHbEGwIaDE",
        "colab_type": "code",
        "outputId": "d86be7a7-9b01-4b80-f1fe-5cc45742154a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(16, (3, 3), activation='relu', input_shape=(28,28,1))) # 26\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 24\n",
        "model.add(Convolution2D(32, (3, 3), activation='relu')) # 22\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # 11\n",
        "model.add(Convolution2D(16, (1, 1), activation='relu')) # 11\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 9\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 7\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu'))#5\n",
        "\n",
        "model.add(Convolution2D(16, 1, activation='relu')) #5\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0829 06:25:29.252846 139715468093312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0829 06:25:29.289044 139715468093312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0829 06:25:29.295515 139715468093312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0829 06:25:29.348824 139715468093312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkFtLWcisTy3",
        "colab_type": "code",
        "outputId": "deeb2207-f715-4713-a4a9-df86983482cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 16)        2320      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 16)        528       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 5, 5, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 5, 5, 16)          272       \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 1, 1, 10)          4010      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 18,890\n",
            "Trainable params: 18,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jql1KrlIalv",
        "colab_type": "code",
        "outputId": "b7379afa-4977-4ac1-84c8-a54ff2cab5a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=30, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0829 06:25:29.466509 139715468093312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0829 06:25:29.494266 139715468093312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0829 06:25:29.751634 139715468093312 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0829 06:25:29.870695 139715468093312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 14s 236us/step - loss: 0.3485 - acc: 0.8876 - val_loss: 0.1047 - val_acc: 0.9666\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0972 - acc: 0.9704 - val_loss: 0.0656 - val_acc: 0.9786\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0708 - acc: 0.9783 - val_loss: 0.0502 - val_acc: 0.9834\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0565 - acc: 0.9827 - val_loss: 0.0593 - val_acc: 0.9815\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0491 - acc: 0.9845 - val_loss: 0.0498 - val_acc: 0.9847\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0437 - acc: 0.9870 - val_loss: 0.0366 - val_acc: 0.9885\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0400 - acc: 0.9876 - val_loss: 0.0369 - val_acc: 0.9890\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0356 - acc: 0.9888 - val_loss: 0.0385 - val_acc: 0.9878\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0325 - acc: 0.9901 - val_loss: 0.0426 - val_acc: 0.9869\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0305 - acc: 0.9903 - val_loss: 0.0333 - val_acc: 0.9901\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0277 - acc: 0.9914 - val_loss: 0.0425 - val_acc: 0.9880\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0256 - acc: 0.9920 - val_loss: 0.0367 - val_acc: 0.9889\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0247 - acc: 0.9921 - val_loss: 0.0330 - val_acc: 0.9901\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0221 - acc: 0.9930 - val_loss: 0.0312 - val_acc: 0.9907\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0212 - acc: 0.9929 - val_loss: 0.0303 - val_acc: 0.9908\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0193 - acc: 0.9935 - val_loss: 0.0304 - val_acc: 0.9908\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0186 - acc: 0.9939 - val_loss: 0.0283 - val_acc: 0.9919\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0177 - acc: 0.9942 - val_loss: 0.0359 - val_acc: 0.9906\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0150 - acc: 0.9955 - val_loss: 0.0396 - val_acc: 0.9889\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0142 - acc: 0.9955 - val_loss: 0.0255 - val_acc: 0.9921\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0134 - acc: 0.9955 - val_loss: 0.0359 - val_acc: 0.9902\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0140 - acc: 0.9954 - val_loss: 0.0347 - val_acc: 0.9894\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0131 - acc: 0.9954 - val_loss: 0.0289 - val_acc: 0.9922\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0118 - acc: 0.9960 - val_loss: 0.0354 - val_acc: 0.9908\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0108 - acc: 0.9964 - val_loss: 0.0410 - val_acc: 0.9890\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0120 - acc: 0.9959 - val_loss: 0.0367 - val_acc: 0.9897\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0099 - acc: 0.9966 - val_loss: 0.0329 - val_acc: 0.9913\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0103 - acc: 0.9966 - val_loss: 0.0310 - val_acc: 0.9913\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0112 - acc: 0.9961 - val_loss: 0.0327 - val_acc: 0.9910\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0363 - val_acc: 0.9899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f11b03b30f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d6bo4ChCOx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6748fbe1-f08a-40e3-e0da-773144a8200e"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.03627348785162321, 0.9899]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3lfgqreNIFC",
        "colab_type": "text"
      },
      "source": [
        "## Observation\n",
        "\n",
        "Achieved Validation Accuracy: 98.85, but Training Accuracy was 99.68.\n",
        "\n",
        "Model seems to overfit the training data.\n",
        "\n",
        "Few regularizations and batch normalization could be introduced to generalize the data better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT3akde6Mt3O",
        "colab_type": "text"
      },
      "source": [
        "#**Iteration 2**\n",
        "##**Improvement**\n",
        "\n",
        "*   Batch Normalization\n",
        "    - We have standardized our inputs by dividing it by 255, but when we are convolving the values again in our network, we are getting values which are not ranging from 0 to 1. Different layers are getting values in different range. So its better to introduce Batch normalization after we perform our convolution each time.\n",
        "    \n",
        "    - Moreover during backpropagation, neural networks can be generalized better when the distribution of values are normalized.\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "eccbba3c-3543-4bfd-99e7-50bfe4cd7511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(16, (3, 3), activation='relu', input_shape=(28,28,1))) # 26\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(32, (3, 3), activation='relu')) # 22\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # 11\n",
        "model.add(Convolution2D(16, (1, 1), activation='relu')) # 11\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 5\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, 1, activation='relu')) #7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0829 06:29:14.334406 139715468093312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "547aa54f-5121-49eb-da7b-576187cc415c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, batch_size=32, epochs=30, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 24, 24, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 22, 22, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 22, 22, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 11, 11, 16)        528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 9, 9, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 5, 5, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 5, 5, 16)          272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 1, 1, 10)          4010      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 19,466\n",
            "Trainable params: 19,178\n",
            "Non-trainable params: 288\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 41s 689us/step - loss: 0.1663 - acc: 0.9491 - val_loss: 0.0592 - val_acc: 0.9819\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 40s 661us/step - loss: 0.0546 - acc: 0.9830 - val_loss: 0.0384 - val_acc: 0.9882\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 39s 657us/step - loss: 0.0417 - acc: 0.9869 - val_loss: 0.0369 - val_acc: 0.9886\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 39s 654us/step - loss: 0.0362 - acc: 0.9889 - val_loss: 0.0361 - val_acc: 0.9884\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 39s 651us/step - loss: 0.0297 - acc: 0.9909 - val_loss: 0.0405 - val_acc: 0.9874\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 39s 656us/step - loss: 0.0274 - acc: 0.9910 - val_loss: 0.0377 - val_acc: 0.9891\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 39s 654us/step - loss: 0.0246 - acc: 0.9914 - val_loss: 0.0353 - val_acc: 0.9893\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 39s 654us/step - loss: 0.0195 - acc: 0.9939 - val_loss: 0.0334 - val_acc: 0.9896\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 39s 656us/step - loss: 0.0202 - acc: 0.9934 - val_loss: 0.0439 - val_acc: 0.9874\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 40s 668us/step - loss: 0.0175 - acc: 0.9945 - val_loss: 0.0384 - val_acc: 0.9887\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 39s 655us/step - loss: 0.0154 - acc: 0.9949 - val_loss: 0.0302 - val_acc: 0.9913\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 39s 651us/step - loss: 0.0150 - acc: 0.9949 - val_loss: 0.0313 - val_acc: 0.9911\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 40s 659us/step - loss: 0.0126 - acc: 0.9959 - val_loss: 0.0307 - val_acc: 0.9914\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 39s 647us/step - loss: 0.0132 - acc: 0.9956 - val_loss: 0.0319 - val_acc: 0.9905\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 39s 652us/step - loss: 0.0109 - acc: 0.9964 - val_loss: 0.0341 - val_acc: 0.9909\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 39s 651us/step - loss: 0.0107 - acc: 0.9964 - val_loss: 0.0396 - val_acc: 0.9896\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 39s 654us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0379 - val_acc: 0.9888\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 39s 658us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0377 - val_acc: 0.9904\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 39s 654us/step - loss: 0.0098 - acc: 0.9968 - val_loss: 0.0364 - val_acc: 0.9898\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 39s 653us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0433 - val_acc: 0.9892\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 39s 651us/step - loss: 0.0078 - acc: 0.9973 - val_loss: 0.0408 - val_acc: 0.9899\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 39s 654us/step - loss: 0.0080 - acc: 0.9970 - val_loss: 0.0509 - val_acc: 0.9886\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 39s 655us/step - loss: 0.0077 - acc: 0.9973 - val_loss: 0.0508 - val_acc: 0.9884\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 39s 650us/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0407 - val_acc: 0.9904\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 39s 647us/step - loss: 0.0067 - acc: 0.9975 - val_loss: 0.0392 - val_acc: 0.9910\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 39s 656us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 0.0376 - val_acc: 0.9906\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 39s 651us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0363 - val_acc: 0.9915\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 39s 648us/step - loss: 0.0057 - acc: 0.9980 - val_loss: 0.0364 - val_acc: 0.9918\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 39s 650us/step - loss: 0.0060 - acc: 0.9978 - val_loss: 0.0463 - val_acc: 0.9900\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 39s 654us/step - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0504 - val_acc: 0.9895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f11a0678390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY0lJgRTCcmY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41bfb7a0-0d14-4094-e48e-126d6a40ed44"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0) \n",
        "print(score)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.05036503328697222, 0.9895]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHY6ZqMBPm3v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "##Observation:\n",
        "\n",
        "\n",
        "1.   Adding Batch Normalization increases the accuracy meanwhile it increases the epoch time\n",
        "2.   But the gap between the test accuracy(99.31) and train accuracy(99.72) is more\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3AOJj3ZGl6w",
        "colab_type": "text"
      },
      "source": [
        "#**Iteration 3**\n",
        "\n",
        "*  Adding any regularization method, will help in avoiding overfitting the data, Adding Dropout in the next iteration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JTDd0DuPsf2",
        "colab_type": "code",
        "outputId": "c0b54408-6f14-4668-93a6-a7d53c39942a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1))) # 26\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) # 24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu')) # 22\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # 11\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu')) # 11\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) # 9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu')) # 7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Convolution2D(10, 1, activation='relu')) #7\n",
        "\n",
        "model.add(Convolution2D(10, 7))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "W0829 06:48:57.798825 139715468093312 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWoQoGV4dBJC",
        "colab_type": "code",
        "outputId": "e5aa7187-bcea-4347-f59e-e9d4b036df40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, batch_size=128, nb_epoch=30, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 22, 22, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 22, 22, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 22, 22, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 11, 11, 10)        330       \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 11, 11, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 9, 9, 16)          1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 7, 7, 32)          4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 7, 7, 10)          330       \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 1, 1, 10)          4910      \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 18,326\n",
            "Trainable params: 18,094\n",
            "Non-trainable params: 232\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 13s 221us/step - loss: 0.2646 - acc: 0.9154 - val_loss: 0.0692 - val_acc: 0.9771\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.0633 - acc: 0.9806 - val_loss: 0.0452 - val_acc: 0.9855\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0485 - acc: 0.9850 - val_loss: 0.0476 - val_acc: 0.9852\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0381 - acc: 0.9883 - val_loss: 0.0361 - val_acc: 0.9882\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0334 - acc: 0.9893 - val_loss: 0.0332 - val_acc: 0.9888\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0294 - acc: 0.9907 - val_loss: 0.0338 - val_acc: 0.9895\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0276 - acc: 0.9907 - val_loss: 0.0372 - val_acc: 0.9886\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0255 - acc: 0.9915 - val_loss: 0.0347 - val_acc: 0.9891\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0233 - acc: 0.9926 - val_loss: 0.0362 - val_acc: 0.9889\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0210 - acc: 0.9933 - val_loss: 0.0294 - val_acc: 0.9900\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0211 - acc: 0.9930 - val_loss: 0.0353 - val_acc: 0.9888\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.0202 - acc: 0.9933 - val_loss: 0.0272 - val_acc: 0.9924\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0178 - acc: 0.9939 - val_loss: 0.0283 - val_acc: 0.9915\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0171 - acc: 0.9948 - val_loss: 0.0292 - val_acc: 0.9911\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0167 - acc: 0.9946 - val_loss: 0.0305 - val_acc: 0.9896\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0154 - acc: 0.9945 - val_loss: 0.0288 - val_acc: 0.9916\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0144 - acc: 0.9952 - val_loss: 0.0399 - val_acc: 0.9887\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0142 - acc: 0.9955 - val_loss: 0.0258 - val_acc: 0.9917\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0135 - acc: 0.9956 - val_loss: 0.0318 - val_acc: 0.9906\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0122 - acc: 0.9959 - val_loss: 0.0308 - val_acc: 0.9903\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0126 - acc: 0.9957 - val_loss: 0.0302 - val_acc: 0.9912\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 11s 183us/step - loss: 0.0116 - acc: 0.9963 - val_loss: 0.0333 - val_acc: 0.9893\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0115 - acc: 0.9961 - val_loss: 0.0316 - val_acc: 0.9915\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.0118 - acc: 0.9958 - val_loss: 0.0299 - val_acc: 0.9919\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0100 - acc: 0.9967 - val_loss: 0.0322 - val_acc: 0.9904\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.0095 - acc: 0.9969 - val_loss: 0.0376 - val_acc: 0.9896\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0343 - val_acc: 0.9897\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0353 - val_acc: 0.9904\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0395 - val_acc: 0.9898\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0395 - val_acc: 0.9901\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f115ffe3390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF_dtO8uCnKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "979bca6c-af65-4cfe-9c49-c4451303e284"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.03951352105533265, 0.9901]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mh4WeShJljM",
        "colab_type": "text"
      },
      "source": [
        "##Observation:\n",
        "\n",
        "\n",
        "1.   Adding Batch Normalization increases the accuracy meanwhile it increases the epoch time\n",
        "2.   But the gap between the test accuracy(99.31) and train accuracy(99.72) is more\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4WhKLZxQL2D",
        "colab_type": "text"
      },
      "source": [
        "#**Iteration 4**\n",
        "\n",
        "*    Reducing paramters under 15K - Since its a small dataset, reduing the number of kernels used.\n",
        "*   Instead of adding a dropout of 0.25, more dropouts of 0.1 are introduced. \n",
        "*   Learning rate - a hyperparameter which tells how much to  the model weights can be updated in response to the loss\n",
        "\n",
        "![Learning Rate](https://github.com/gkdivya/EVA/blob/master/Architectural%20Basics/assets/learning%20rate.png?raw=true)\n",
        "*   Instead of a constant learning rate, step based learning rate schedules are better.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5UdZXoDQOTE",
        "colab_type": "code",
        "outputId": "9ab4f539-3fa0-4abd-d08f-1e81b37c0d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Convolution2D(10, (3, 3), activation='relu', input_shape=(28,28,1))) #26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) #24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu'))#22\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "model.add(Convolution2D(10, (1, 1), activation='relu')) #11\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu'))#9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu'))#7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu'))#5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, (5, 5)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_44 (Conv2D)           (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 24, 24, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 22, 22, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 11, 11, 10)        170       \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 9, 9, 16)          1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 5, 5, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 1, 1, 10)          4010      \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,512\n",
            "Trainable params: 14,332\n",
            "Non-trainable params: 180\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N71UmkJnVlql",
        "colab_type": "code",
        "outputId": "52b9d9a2-c2c6-4831-f4b4-495264199362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "callbacks=[LearningRateScheduler(scheduler, verbose=1)]\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, callbacks=callbacks, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 14s 232us/step - loss: 0.2100 - acc: 0.9325 - val_loss: 0.0696 - val_acc: 0.9773\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0627 - acc: 0.9803 - val_loss: 0.0498 - val_acc: 0.9835\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0480 - acc: 0.9847 - val_loss: 0.0351 - val_acc: 0.9891\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0403 - acc: 0.9872 - val_loss: 0.0287 - val_acc: 0.9904\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0354 - acc: 0.9888 - val_loss: 0.0271 - val_acc: 0.9920\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0336 - acc: 0.9896 - val_loss: 0.0247 - val_acc: 0.9917\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0292 - acc: 0.9905 - val_loss: 0.0234 - val_acc: 0.9923\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0262 - acc: 0.9917 - val_loss: 0.0208 - val_acc: 0.9930\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 10s 168us/step - loss: 0.0247 - acc: 0.9917 - val_loss: 0.0190 - val_acc: 0.9933\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0249 - acc: 0.9921 - val_loss: 0.0209 - val_acc: 0.9931\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0219 - acc: 0.9929 - val_loss: 0.0217 - val_acc: 0.9931\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0224 - acc: 0.9929 - val_loss: 0.0192 - val_acc: 0.9936\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0207 - acc: 0.9933 - val_loss: 0.0213 - val_acc: 0.9929\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0203 - acc: 0.9935 - val_loss: 0.0191 - val_acc: 0.9938\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0174 - acc: 0.9947 - val_loss: 0.0180 - val_acc: 0.9943\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0182 - acc: 0.9943 - val_loss: 0.0180 - val_acc: 0.9944\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0172 - acc: 0.9944 - val_loss: 0.0191 - val_acc: 0.9940\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0178 - acc: 0.9943 - val_loss: 0.0177 - val_acc: 0.9946\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0167 - acc: 0.9947 - val_loss: 0.0163 - val_acc: 0.9945\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0155 - acc: 0.9950 - val_loss: 0.0174 - val_acc: 0.9940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f115ebcbe80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5rxr7PqXptz",
        "colab_type": "code",
        "outputId": "c34d27f4-a887-4dfe-e408-ecd7efa0ca8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.017445057881696993, 0.994]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbKbauIgudAK",
        "colab_type": "text"
      },
      "source": [
        "##Observation:\n",
        "\n",
        "\n",
        "1.   Adding stepwise learning rate increases the validation accuracy as 99.4\n",
        "2.   Moreover the gap between the test accuracy(99.50) and train accuracy(99.40) is reduced.\n"
      ]
    }
  ]
}