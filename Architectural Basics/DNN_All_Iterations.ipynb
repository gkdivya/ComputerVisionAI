{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArchitecturalBasics_DNN_AllIterations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkdivya/EVA/blob/master/Architectural%20Basics/DNN_All_Iterations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Architectural Basics**\n",
        "##**Building a deep Neural network**\n",
        "\n",
        "In this notebook, let's see how to build a neural network from scratch for MNIST image classification and how we can tune our architecture to optimize our network better step by step.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvXkwJutuOYj",
        "colab_type": "text"
      },
      "source": [
        "**Import Libraries and Modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "6ca06400-11af-4b47-8c31-548331a4b6e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "# Importing Keras library\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjlTGDq8O2LP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# Importing Keras classes used for building CNN\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "# Importing Keras Inbuilt dataset\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "outputId": "ffec028f-87f7-452d-dda1-4a66774476d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "7eff805e-f717-4e35-fd4d-1e95df03806b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "#Visualize one image in training set\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3dc3987f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBX30JkCLaOt",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping to hold the no of channels\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data converted to float and performed Image standardization\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "b002edee-0593-4eef-aa98-a0fd8cbd6f08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Train Labels\n",
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu01vby64HNv",
        "colab_type": "text"
      },
      "source": [
        "**One Hot Encoding**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "7594357f-85e9-4c8d-d32f-b5360cca727a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Train Labels after one hot encoding, for example 5 is converted as [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]\n",
        "Y_train[:10]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_6PykdOstzq",
        "colab_type": "text"
      },
      "source": [
        "#**Iteration 1**\n",
        "##**How we define a vanila neural network for our classification problem**\n",
        "\n",
        "**Convolution Blocks & Transition Blocks**\n",
        "*   We will start building our network by deciding how many convolution blocks we need based on our input image size and the size of the object we are trying to predict in the image\n",
        "*   And we will decide where we will place our transition blocks (Max pooling and 1x1 block) in our architecture\n",
        "*   We will have convolution blocks followed by transition blocks till we reach our recptive field. \n",
        "\n",
        "**Kernels**\n",
        "*   We will decide the number of kernels used \n",
        "*   We keep increasing our no of kernels until we perform max pooling followed by pointwise convolution\n",
        "\n",
        "**Max Pooling**\n",
        "*   We make sure max pooling is 2 or more layers away from output\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1lHbEGwIaDE",
        "colab_type": "code",
        "outputId": "ce4ca7aa-1b1e-455b-df1e-00803462b84f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(16, (3, 3), activation='relu', input_shape=(28,28,1))) # 26\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 24\n",
        "model.add(Convolution2D(32, (3, 3), activation='relu')) # 22\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # 11\n",
        "model.add(Convolution2D(16, (1, 1), activation='relu')) # 11\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 9\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 7\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu'))#5\n",
        "\n",
        "model.add(Convolution2D(16, 1, activation='relu')) #5\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0828 16:34:40.561984 139904771368832 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0828 16:34:40.611366 139904771368832 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0828 16:34:40.621373 139904771368832 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0828 16:34:40.701303 139904771368832 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkFtLWcisTy3",
        "colab_type": "code",
        "outputId": "b4cae0ea-a69a-4e59-aac6-45789eb28c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 16)        2320      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 16)        528       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 5, 5, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 5, 5, 16)          272       \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 1, 1, 10)          4010      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 18,890\n",
            "Trainable params: 18,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jql1KrlIalv",
        "colab_type": "code",
        "outputId": "a21dcd61-0dbf-43d3-bbcc-5c69b8c1cdb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=30, verbose=1)\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0828 16:34:40.832998 139904771368832 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0828 16:34:40.875138 139904771368832 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0828 16:34:41.090123 139904771368832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0828 16:34:41.198498 139904771368832 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 16s 259us/step - loss: 0.3590 - acc: 0.8842\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.1020 - acc: 0.9693\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0712 - acc: 0.9782\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0592 - acc: 0.9818\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0481 - acc: 0.9853\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0424 - acc: 0.9869\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0380 - acc: 0.9881\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0343 - acc: 0.9892\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0310 - acc: 0.9901\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0287 - acc: 0.9907\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0241 - acc: 0.9919\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0251 - acc: 0.9921\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0225 - acc: 0.9927\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0209 - acc: 0.9933\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0206 - acc: 0.9932\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0182 - acc: 0.9943\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0176 - acc: 0.9943\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0159 - acc: 0.9947\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0151 - acc: 0.9951\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0136 - acc: 0.9952\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0142 - acc: 0.9950\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0132 - acc: 0.9954\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0119 - acc: 0.9957\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.0123 - acc: 0.9956\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0110 - acc: 0.9962\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0113 - acc: 0.9963\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0105 - acc: 0.9965\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0083 - acc: 0.9972\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0106 - acc: 0.9965\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0087 - acc: 0.9973\n",
            "[0.04175897129344139, 0.9897]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3lfgqreNIFC",
        "colab_type": "text"
      },
      "source": [
        "## Observation\n",
        "\n",
        "Achieved Validation Accuracy: 98.85, but Training Accuracy was 99.68.\n",
        "\n",
        "Model seems to overfit the training data.\n",
        "\n",
        "Few regularizations and batch normalization could be introduced to generalize the data better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT3akde6Mt3O",
        "colab_type": "text"
      },
      "source": [
        "#**Iteration 2**\n",
        "##**Improvement**\n",
        "\n",
        "*   Batch Normalization\n",
        "    - We have standardized our inputs by dividing it by 255, but when we are convolving the values again in our network, we are getting values which are not ranging from 0 to 1. Different layers are getting values in different range. So its better to introduce Batch normalization after we perform our convolution each time.\n",
        "    \n",
        "    - Moreover during backpropagation, neural networks can be generalized better when the distribution of values are normalized.\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "de53c8b8-f727-466b-9ef4-d8c6ab4e0354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(16, (3, 3), activation='relu', input_shape=(28,28,1))) # 26\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(32, (3, 3), activation='relu')) # 22\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # 11\n",
        "model.add(Convolution2D(16, (1, 1), activation='relu')) # 11\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu')) # 5\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, 1, activation='relu')) #7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0828 16:38:19.412393 139904771368832 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "f4642b74-ab8c-4faf-fc40-e6e1998e5f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, batch_size=32, epochs=30, verbose=1)\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 24, 24, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 22, 22, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 22, 22, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 11, 11, 16)        528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 9, 9, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 5, 5, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 5, 5, 16)          272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 1, 1, 10)          4010      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 19,466\n",
            "Trainable params: 19,178\n",
            "Non-trainable params: 288\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 39s 653us/step - loss: 0.1548 - acc: 0.9515\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 37s 623us/step - loss: 0.0507 - acc: 0.9842\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 38s 625us/step - loss: 0.0409 - acc: 0.9872\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 37s 624us/step - loss: 0.0352 - acc: 0.9890\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 38s 626us/step - loss: 0.0311 - acc: 0.9902\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 37s 624us/step - loss: 0.0263 - acc: 0.9915\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 38s 625us/step - loss: 0.0234 - acc: 0.9927\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 37s 625us/step - loss: 0.0215 - acc: 0.9929\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 37s 621us/step - loss: 0.0207 - acc: 0.9929\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 38s 629us/step - loss: 0.0165 - acc: 0.9947\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 37s 623us/step - loss: 0.0164 - acc: 0.9944\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 37s 622us/step - loss: 0.0147 - acc: 0.9952\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 37s 623us/step - loss: 0.0126 - acc: 0.9958\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 37s 622us/step - loss: 0.0128 - acc: 0.9958\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 37s 623us/step - loss: 0.0119 - acc: 0.9962\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 37s 624us/step - loss: 0.0105 - acc: 0.9965\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 37s 621us/step - loss: 0.0098 - acc: 0.9968\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 38s 628us/step - loss: 0.0102 - acc: 0.9966\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 37s 622us/step - loss: 0.0080 - acc: 0.9973\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 37s 623us/step - loss: 0.0085 - acc: 0.9971\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 37s 623us/step - loss: 0.0079 - acc: 0.9972\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 37s 621us/step - loss: 0.0074 - acc: 0.9973\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 37s 623us/step - loss: 0.0085 - acc: 0.9974\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 38s 628us/step - loss: 0.0065 - acc: 0.9977\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 38s 625us/step - loss: 0.0075 - acc: 0.9975\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 38s 631us/step - loss: 0.0053 - acc: 0.9981\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 37s 623us/step - loss: 0.0060 - acc: 0.9980\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 37s 624us/step - loss: 0.0064 - acc: 0.9978\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 38s 625us/step - loss: 0.0061 - acc: 0.9979\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 37s 622us/step - loss: 0.0056 - acc: 0.9980\n",
            "[0.04019097753570204, 0.9905]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHY6ZqMBPm3v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "##Observation:\n",
        "\n",
        "\n",
        "1.   Adding Batch Normalization increases the accuracy meanwhile it increases the epoch time\n",
        "2.   But the gap between the test accuracy(99.31) and train accuracy(99.72) is more\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3AOJj3ZGl6w",
        "colab_type": "text"
      },
      "source": [
        "#**Iteration 3**\n",
        "\n",
        "*  Adding any regularization method, will help in avoiding overfitting the data, Adding Dropout in the next iteration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JTDd0DuPsf2",
        "colab_type": "code",
        "outputId": "5b824a0d-5561-4035-f27b-2f8e81da80d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1))) # 26\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) # 24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu')) # 22\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # 11\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu')) # 11\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) # 9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu')) # 7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Convolution2D(10, 1, activation='relu')) #7\n",
        "\n",
        "model.add(Convolution2D(10, 7))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "W0828 17:02:28.351738 139904771368832 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWoQoGV4dBJC",
        "colab_type": "code",
        "outputId": "348dcfec-2fc4-47b8-844c-209219310563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, batch_size=128, nb_epoch=30, verbose=1)\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 22, 22, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 22, 22, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 22, 22, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 11, 11, 10)        330       \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 11, 11, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 9, 9, 16)          1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 7, 7, 32)          4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 7, 7, 10)          330       \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 1, 1, 10)          4910      \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 18,326\n",
            "Trainable params: 18,094\n",
            "Non-trainable params: 232\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 12s 204us/step - loss: 0.2680 - acc: 0.9152\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0646 - acc: 0.9805\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0482 - acc: 0.9850\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0411 - acc: 0.9866\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0350 - acc: 0.9889\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0298 - acc: 0.9901\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0295 - acc: 0.9904\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0260 - acc: 0.9914\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0233 - acc: 0.9927\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0235 - acc: 0.9921\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0211 - acc: 0.9932\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0196 - acc: 0.9937\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0183 - acc: 0.9937\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.0168 - acc: 0.9942\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0166 - acc: 0.9945\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0168 - acc: 0.9942\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0152 - acc: 0.9948\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0165 - acc: 0.9943\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0131 - acc: 0.9956\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0132 - acc: 0.9954\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0133 - acc: 0.9956\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0121 - acc: 0.9959\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0108 - acc: 0.9964\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0113 - acc: 0.9960\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0116 - acc: 0.9960\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0116 - acc: 0.9960\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0097 - acc: 0.9967\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0092 - acc: 0.9968\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0098 - acc: 0.9970\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0097 - acc: 0.9968\n",
            "[0.02944388747494022, 0.9912]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "5f185ebc-e880-4d77-e250-0d2b174e2d0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.58555454e-14 9.48063676e-13 2.90500735e-09 1.34341106e-11\n",
            "  1.35577487e-18 2.92705682e-15 1.00463815e-22 1.00000000e+00\n",
            "  5.80136593e-14 1.74355475e-10]\n",
            " [3.97990335e-10 1.40519516e-08 9.99998927e-01 6.31750430e-10\n",
            "  9.61809634e-18 3.05956143e-19 1.12694909e-06 3.17335168e-14\n",
            "  2.20181967e-12 2.13421697e-18]\n",
            " [7.58922647e-11 9.99999881e-01 1.24514588e-09 8.95473105e-12\n",
            "  5.96020733e-10 3.40553452e-09 3.10150376e-08 1.18460562e-07\n",
            "  6.02802641e-10 7.01641037e-11]\n",
            " [1.00000000e+00 2.26552430e-15 1.43059439e-10 4.00222661e-14\n",
            "  6.07058730e-15 4.56064562e-14 1.61832805e-08 1.00587712e-14\n",
            "  1.18887734e-12 4.06438034e-12]\n",
            " [2.25019398e-13 3.74528436e-10 2.07378364e-12 2.42333111e-14\n",
            "  9.99999404e-01 4.85503351e-16 2.88662670e-11 6.85269549e-11\n",
            "  3.12276122e-10 6.55068504e-07]\n",
            " [3.28769347e-13 9.99999642e-01 6.96064206e-10 1.36352152e-13\n",
            "  1.40982032e-10 1.20009918e-13 1.98909458e-10 3.18999014e-07\n",
            "  2.17447345e-11 1.38789345e-11]\n",
            " [3.45235905e-18 4.47590798e-10 1.42565508e-11 5.17153662e-14\n",
            "  9.99992251e-01 1.24168552e-13 3.63141253e-14 1.38119958e-07\n",
            "  6.85250734e-06 7.76850300e-07]\n",
            " [3.92726823e-12 1.23023369e-12 1.75774939e-09 4.91477248e-10\n",
            "  3.59886513e-07 8.64772576e-12 3.14578956e-14 9.56381085e-10\n",
            "  3.07016741e-08 9.99999642e-01]\n",
            " [3.16793168e-13 2.30038535e-18 7.08318014e-16 2.00691265e-12\n",
            "  3.45402028e-21 9.99997973e-01 1.58637454e-06 2.67409708e-19\n",
            "  4.78444349e-07 1.33527703e-11]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mh4WeShJljM",
        "colab_type": "text"
      },
      "source": [
        "##Observation:\n",
        "\n",
        "\n",
        "1.   Adding Batch Normalization increases the accuracy meanwhile it increases the epoch time\n",
        "2.   But the gap between the test accuracy(99.31) and train accuracy(99.72) is more\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4WhKLZxQL2D",
        "colab_type": "text"
      },
      "source": [
        "#**Iteration 4**\n",
        "\n",
        "*    Reducing paramters under 15K - Since its a small dataset, reduing the number of kernels used\n",
        "*   Instead of adding a dropout of 0.25, more dropouts of 0.1 are introduced. \n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5UdZXoDQOTE",
        "colab_type": "code",
        "outputId": "93019364-4e04-49d2-c768-50b08742bc32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1))) #26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) #24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))#22\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 1, 1, activation='relu')) #22\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu'))#9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 1, 1, activation='relu')) #9\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))#7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))#5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, 5, 5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_96 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_97 (Conv2D)           (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_98 (Conv2D)           (None, 22, 22, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_51 (Dropout)         (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_99 (Conv2D)           (None, 22, 22, 16)        272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_100 (Conv2D)          (None, 9, 9, 8)           1160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 9, 9, 8)           32        \n",
            "_________________________________________________________________\n",
            "dropout_52 (Dropout)         (None, 9, 9, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_101 (Conv2D)          (None, 9, 9, 16)          144       \n",
            "_________________________________________________________________\n",
            "conv2d_102 (Conv2D)          (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_103 (Conv2D)          (None, 5, 5, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_104 (Conv2D)          (None, 1, 1, 10)          4010      \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,114\n",
            "Trainable params: 13,954\n",
            "Non-trainable params: 160\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (5, 5))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N71UmkJnVlql",
        "colab_type": "code",
        "outputId": "971018d9-ca2e-4990-8481-f7ee19700884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "callbacks=[LearningRateScheduler(scheduler, verbose=1)]\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=30, verbose=1, callbacks=callbacks, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.2271 - acc: 0.9272 - val_loss: 0.0651 - val_acc: 0.9791\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 12s 201us/step - loss: 0.0673 - acc: 0.9797 - val_loss: 0.0704 - val_acc: 0.9771\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0496 - acc: 0.9844 - val_loss: 0.0335 - val_acc: 0.9897\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0428 - acc: 0.9866 - val_loss: 0.0346 - val_acc: 0.9886\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0381 - acc: 0.9881 - val_loss: 0.0305 - val_acc: 0.9909\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0365 - acc: 0.9884 - val_loss: 0.0286 - val_acc: 0.9911\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0321 - acc: 0.9899 - val_loss: 0.0267 - val_acc: 0.9918\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0293 - acc: 0.9904 - val_loss: 0.0288 - val_acc: 0.9909\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0287 - acc: 0.9907 - val_loss: 0.0293 - val_acc: 0.9912\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0263 - acc: 0.9915 - val_loss: 0.0258 - val_acc: 0.9924\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0248 - acc: 0.9920 - val_loss: 0.0347 - val_acc: 0.9897\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0237 - acc: 0.9923 - val_loss: 0.0264 - val_acc: 0.9917\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 12s 194us/step - loss: 0.0221 - acc: 0.9928 - val_loss: 0.0277 - val_acc: 0.9923\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0228 - acc: 0.9926 - val_loss: 0.0311 - val_acc: 0.9905\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0213 - acc: 0.9932 - val_loss: 0.0251 - val_acc: 0.9926\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0203 - acc: 0.9931 - val_loss: 0.0256 - val_acc: 0.9927\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0201 - acc: 0.9936 - val_loss: 0.0249 - val_acc: 0.9927\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0201 - acc: 0.9941 - val_loss: 0.0233 - val_acc: 0.9933\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0188 - acc: 0.9938 - val_loss: 0.0266 - val_acc: 0.9925\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0179 - acc: 0.9941 - val_loss: 0.0258 - val_acc: 0.9927\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0179 - acc: 0.9940 - val_loss: 0.0220 - val_acc: 0.9935\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0174 - acc: 0.9940 - val_loss: 0.0267 - val_acc: 0.9931\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0164 - acc: 0.9946 - val_loss: 0.0265 - val_acc: 0.9924\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0164 - acc: 0.9946 - val_loss: 0.0242 - val_acc: 0.9935\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0174 - acc: 0.9941 - val_loss: 0.0286 - val_acc: 0.9927\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0165 - acc: 0.9948 - val_loss: 0.0236 - val_acc: 0.9937\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0155 - acc: 0.9950 - val_loss: 0.0250 - val_acc: 0.9927\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0155 - acc: 0.9947 - val_loss: 0.0235 - val_acc: 0.9934\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0144 - acc: 0.9954 - val_loss: 0.0271 - val_acc: 0.9922\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0145 - acc: 0.9949 - val_loss: 0.0258 - val_acc: 0.9924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3d6bc0eac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5rxr7PqXptz",
        "colab_type": "code",
        "outputId": "4b4b978b-5ac4-4c40-c784-135fe9fabd05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.025836447957489874, 0.9924]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS9dpsHJSgV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}